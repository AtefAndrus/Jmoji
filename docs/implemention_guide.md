# å®Ÿè£…ã‚¬ã‚¤ãƒ‰

## 1. ç’°å¢ƒæ§‹ç¯‰

### 1.1 ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒ

```bash
# ãƒªãƒã‚¸ãƒˆãƒªã‚¯ãƒ­ãƒ¼ãƒ³
git clone https://github.com/AtefAndrus/Jmoji.git
cd Jmoji

# mise ã§ãƒ„ãƒ¼ãƒ«ã‚’å–å¾—ï¼ˆPython 3.12 / uv latestï¼‰
mise install

# uv ã§ä¾å­˜é–¢ä¿‚åŒæœŸï¼ˆ.venv ã¨ uv.lock ã‚’ç”Ÿæˆï¼‰
UV_CACHE_DIR=.uv-cache uv sync

# pip äº’æ›ã®è¦ä»¶ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›¸ãå‡ºã™å ´åˆ
uv export --format requirements-txt > requirements.txt

# å¿…è¦ãªã‚‰ .venv ã‚’æœ‰åŠ¹åŒ–
source .venv/bin/activate  # Linux/Mac
# .venv\Scripts\activate   # Windows

# ç’°å¢ƒå¤‰æ•°è¨­å®š
cp .env.example .env
# .env ã‚’ç·¨é›†
```

### 1.2 Google Colab

```python
# ãƒªãƒã‚¸ãƒˆãƒªã‚¯ãƒ­ãƒ¼ãƒ³
!git clone https://github.com/AtefAndrus/Jmoji.git
%cd Jmoji

# uv ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦åŒæœŸï¼ˆColab ã¯ã‚·ã‚¹ãƒ†ãƒ  Python ã‚’åˆ©ç”¨ï¼‰
!pip install -q uv
!uv sync --frozen

# ç’°å¢ƒå¤‰æ•°è¨­å®šï¼ˆColabã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆæ¨å¥¨ï¼‰
import os
from google.colab import userdata
os.environ["OPENROUTER_API_KEY"] = userdata.get("OPENROUTER_API_KEY")

# src/ ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¯èƒ½ã«ã™ã‚‹
import sys
sys.path.append("/content/Jmoji")
```

## 2. ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

> **è©³ç´°ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆã®å“è³ªæ”¹å–„ã«ã¤ã„ã¦ã¯ [dataset_generation_v3.md](details/datasets/generation_v3.md) ã‚’å‚ç…§ã€‚

### 2.1 Wikipedia ãƒ‡ãƒ¼ã‚¿å–å¾—

```python
from datasets import load_dataset

# æ—¥æœ¬èªWikipediaï¼ˆç´„1.4Mè¨˜äº‹ï¼‰
ds = load_dataset("wikimedia/wikipedia", "20231101.ja", split="train")

# ã‚µãƒ³ãƒ—ãƒ«ç¢ºèª
print(ds[0])
# {'id': '...', 'url': '...', 'title': '...', 'text': '...'}
```

### 2.2 æ–‡ã®æŠ½å‡ºã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°

```python
import re

def extract_sentences(text: str, min_len: int = 10, max_len: int = 100) -> list[str]:
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ–‡ã‚’æŠ½å‡º"""
    # æ–‡åˆ†å‰²ï¼ˆç°¡æ˜“ç‰ˆï¼‰
    sentences = re.split(r'(?<=[ã€‚ï¼ï¼Ÿ])', text)

    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    filtered = []
    for s in sentences:
        s = s.strip()
        if min_len <= len(s) <= max_len:
            # è¨˜å·ã®ã¿ã€URLã®ã¿ç­‰ã‚’é™¤å¤–
            if re.search(r'[ã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¥]', s):
                filtered.append(s)

    return filtered
```

### 2.3 ãƒ†ã‚­ã‚¹ãƒˆæ­£è¦åŒ–

```python
import unicodedata

def normalize_text(text: str) -> str:
    """ãƒ†ã‚­ã‚¹ãƒˆã®æ­£è¦åŒ–"""
    # NFKCæ­£è¦åŒ–ï¼ˆå…¨è§’è‹±æ•°â†’åŠè§’ã€åŠè§’ã‚«ãƒŠâ†’å…¨è§’ç­‰ï¼‰
    text = unicodedata.normalize("NFKC", text)

    # é€£ç¶šç©ºç™½ã‚’å˜ä¸€ã«
    text = re.sub(r'\s+', ' ', text)

    # å‰å¾Œã®ç©ºç™½é™¤å»
    text = text.strip()

    return text
```

### 2.4 NSFWã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚£ãƒ«ã‚¿

Wikipediaã«ã¯NSFWï¼ˆæ€§çš„ãƒ»æš´åŠ›çš„ï¼‰ãªè¨˜äº‹ãŒå­˜åœ¨ã—ã€Claude APIãŒã“ã‚Œã‚‰ã®å‡¦ç†ã‚’æ‹’å¦ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚
äº‹å‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã§APIã‚³ã‚¹ãƒˆã‚’å‰Šæ¸›ã—ã€æ‹’å¦ç‡ã‚’ç›£è¦–ã™ã‚‹ã€‚

```python
from typing import Optional, Set

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®NSFWã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
DEFAULT_NSFW_KEYWORDS: Set[str] = {
    "æ€§è¡Œç‚º", "æ€§äº¤", "ãƒãƒ«ãƒ", "ã‚¢ãƒ€ãƒ«ãƒˆ", "é¢¨ä¿—",
    "å£²æ˜¥", "æ·«è¡Œ", "æ®ºäºº", "è™æ®º", "æ‹·å•", "å‡¦åˆ‘", "æƒ¨æ®º",
}

def is_safe_sentence(text: str, keywords: Optional[Set[str]] = None) -> bool:
    """NSFWã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã¾ãªã„ã‹ãƒã‚§ãƒƒã‚¯"""
    if keywords is None:
        keywords = DEFAULT_NSFW_KEYWORDS
    return not any(kw in text for kw in keywords)

def filter_safe_sentences(sentences: list[str], keywords: Optional[Set[str]] = None) -> list[str]:
    """NSFWã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€æ–‡ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
    if keywords is None:
        keywords = DEFAULT_NSFW_KEYWORDS
    return [s for s in sentences if is_safe_sentence(s, keywords)]
```

è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ`configs/default.yaml`ï¼‰ã§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½:

```yaml
data:
  nsfw_filter:
    enabled: true
    keywords:
      - "æ€§è¡Œç‚º"
      - "æ®ºäºº"
      # ... å¿…è¦ã«å¿œã˜ã¦è¿½åŠ 
```

### 2.5 æ–‡å®Œå…¨æ€§ãƒ•ã‚£ãƒ«ã‚¿

Wikipediaã®æ–‡åˆ†å‰²ã§ã¯ã€åŠç«¯ãªæ–‡ï¼ˆãƒ¡ã‚¿æƒ…å ±ã€é€”ä¸­ã§åˆ‡ã‚ŒãŸæ–‡ã€é–‰ã˜æ‹¬å¼§ã§å§‹ã¾ã‚‹æ–‡ãªã©ï¼‰ãŒæ··å…¥ã™ã‚‹ã€‚
ã“ã‚Œã‚‰ã‚’Claudeã«æ¸¡ã™å‰ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹ã“ã¨ã§ã€APIå›ç­”æ··å…¥ã‚„SNSå¤‰æ›å¤±æ•—ã‚’é˜²ãã€‚

```python
import re

def is_complete_sentence(text: str) -> tuple[bool, str]:
    """æ–‡ã¨ã—ã¦å®Œå…¨ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
    # ãƒ¡ã‚¿ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆWikipediaã®æ§‹é€ æƒ…å ±ï¼‰
    if re.match(r'^(é–¢é€£é …ç›®|å‚è€ƒæ–‡çŒ®|å¤–éƒ¨ãƒªãƒ³ã‚¯|è„šæ³¨|å‡ºå…¸|æ³¨é‡ˆ)', text):
        return False, "meta_section"
    # é€”ä¸­åˆ‡ã‚Œï¼ˆé–‹ãæ‹¬å¼§ã§çµ‚ã‚ã‚‹ï¼‰
    if re.search(r'[ï¼ˆ(ã€Œã€][^ï¼‰)ã€ã€]{0,30}$', text):
        return False, "truncated"
    # é–‰ã˜æ‹¬å¼§ã§å§‹ã¾ã‚‹ï¼ˆå‰ã®æ–‡è„ˆãŒãªã„ï¼‰
    if re.match(r'^[ã€ã€ï¼‰)]', text):
        return False, "orphan_close"
    # å¥èª­ç‚¹ãªã—
    if not re.search(r'[ã€‚ï¼ï¼Ÿ!?ã€ã€)]$', text):
        return False, "no_ending"
    return True, ""
```

è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã§æœ‰åŠ¹/ç„¡åŠ¹ã‚’åˆ‡ã‚Šæ›¿ãˆå¯èƒ½:

```yaml
data:
  complete_sentence_filter: true  # åŠç«¯ãªæ–‡ã‚’é™¤å¤–ã™ã‚‹ã‹
  buffer_ratio: 1.3               # ä»¶æ•°ä¿è¨¼ã®ãŸã‚ã®ãƒãƒƒãƒ•ã‚¡ç‡
```

### 2.6 ãƒ•ã‚£ãƒ«ã‚¿ãƒ­ã‚°

ãƒ•ã‚£ãƒ«ã‚¿ã§é™¤å¤–ã•ã‚ŒãŸæ–‡ã¯ `data/outputs/filtered_sentences.jsonl` ã«è¨˜éŒ²ã•ã‚Œã‚‹:

```json
{"reason": "nsfw", "detail": "æ®ºäºº", "text": "..."}
{"reason": "incomplete", "detail": "meta_section", "text": "é–¢é€£é …ç›® ..."}
{"reason": "incomplete", "detail": "truncated", "text": "ã€å¤©æ‰ãƒ»ãŸã‘ã—ã®..."}
```

## 3. æ•™å¸«LLMå‘¼ã³å‡ºã—

> **æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã®å±¥æ­´**
>
> - **v1ã€œv3**: Claude Haiku 4.5ï¼ˆOpenRouterçµŒç”±ï¼‰
> - **v4ä»¥é™**: Qwen3-235B-A22Bï¼ˆOpenRouterçµŒç”±ï¼‰
>
> ç§»è¡Œç†ç”±: ã‚³ã‚¹ãƒˆå‰Šæ¸›ï¼ˆç´„1/3ï¼‰ã¨å“è³ªã®ç¶­æŒã€‚è©³ç´°ã¯ [teacher_model_migration.md](details/teacher_model_migration.md) ã‚’å‚ç…§ã€‚

### 3.1 OpenRouter ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ

```python
import os
import httpx
from typing import Optional

class OpenRouterClient:
    def __init__(
        self,
        api_key: Optional[str] = None,
        model: str = "qwen/qwen3-235b-a22b",  # v4ä»¥é™
        base_url: str = "https://openrouter.ai/api/v1"
    ):
        self.api_key = api_key or os.getenv("OPENROUTER_API_KEY")
        self.model = model
        self.base_url = base_url
        self.client = httpx.Client(timeout=60.0)

    def complete(
        self,
        prompt: str,
        temperature: float = 0.7,
        max_tokens: int = 100
    ) -> str:
        response = self.client.post(
            f"{self.base_url}/chat/completions",
            headers={
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            },
            json={
                "model": self.model,
                "messages": [{"role": "user", "content": prompt}],
                "temperature": temperature,
                "max_tokens": max_tokens
            }
        )
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"]
```

### 3.2 ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

```python
# SNSé¢¨æ–‡ä½“å¤‰æ›
SNS_CONVERSION_PROMPT = """ä»¥ä¸‹ã®æ–‡ç« ã‚’ã€æ—¥æœ¬ã®SNSï¼ˆXã€LINEç­‰ï¼‰ã§æŠ•ç¨¿ã•ã‚Œã‚‹ã‚ˆã†ãªã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ãªæ–‡ä½“ã«å¤‰æ›ã—ã¦ãã ã•ã„ã€‚
æ„å‘³ã¯å¤‰ãˆãšã«ã€è©±ã—è¨€è‘‰ã‚„å£èªè¡¨ç¾ã‚’ä½¿ã£ã¦ãã ã•ã„ã€‚
å¤‰æ›å¾Œã®æ–‡ç« ã®ã¿ã‚’å‡ºåŠ›ã—ã€ãã‚Œä»¥å¤–ã¯ä½•ã‚‚å‡ºåŠ›ã—ãªã„ã§ãã ã•ã„ã€‚

å…¥åŠ›: {text}
å‡ºåŠ›:"""

# çµµæ–‡å­—ç”Ÿæˆ
EMOJI_GENERATION_PROMPT = """ä»¥ä¸‹ã®æ—¥æœ¬èªæ–‡ã«å¯¾ã—ã¦ã€æ–‡æœ«ã«ä»˜ä¸ã™ã‚‹ã®ã«é©åˆ‡ãªçµµæ–‡å­—ã‚’1ã€œ5å€‹é¸ã‚“ã§ãã ã•ã„ã€‚

ã€ãƒ«ãƒ¼ãƒ«ã€‘
- çµµæ–‡å­—ã®ã¿ã‚’ç©ºç™½åŒºåˆ‡ã‚Šã§å‡ºåŠ›ã—ã€ãã‚Œä»¥å¤–ã¯ä½•ã‚‚å‡ºåŠ›ã—ãªã„ã§ãã ã•ã„
- âœ¨ï¼ˆã‚­ãƒ©ã‚­ãƒ©ï¼‰ã¯ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„ã€‚ä»–ã®çµµæ–‡å­—ã§è¡¨ç¾ã—ã¦ãã ã•ã„
- æ–‡ã®å…·ä½“çš„ãªå†…å®¹ï¼ˆã‚¹ãƒãƒ¼ãƒ„ã€éŸ³æ¥½ã€é£Ÿã¹ç‰©ã€å‹•ç‰©ãªã©ï¼‰ã«é–¢é€£ã™ã‚‹çµµæ–‡å­—ã‚’å„ªå…ˆã—ã¦ãã ã•ã„
- æ„Ÿæƒ…ã‚’è¡¨ã™å ´åˆã¯é¡”ã®çµµæ–‡å­—ï¼ˆğŸ˜ŠğŸ˜¢ğŸ˜‚ğŸ˜…ğŸ¥ºğŸ˜­ğŸ¤£ãªã©ï¼‰ã‚’ä½¿ã£ã¦ãã ã•ã„
- æ—¥æœ¬ã®SNSï¼ˆXã€LINEãªã©ï¼‰ã§è‡ªç„¶ã«è¦‹ãˆã‚‹ä½¿ã„æ–¹ã‚’æ„è­˜ã—ã¦ãã ã•ã„

å…¥åŠ›: {text}
å‡ºåŠ›:"""
```

**æ³¨æ„**: åˆæœŸå®Ÿé¨“ã§âœ¨ï¼ˆã‚­ãƒ©ã‚­ãƒ©ï¼‰ãŒå…¨çµµæ–‡å­—ã®18.6%ã‚’å ã‚ã€mode collapseã®åŸå› ã¨ãªã£ãŸãŸã‚ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§âœ¨ã®ä½¿ç”¨ã‚’ç¦æ­¢ã—ã¦ã„ã‚‹ã€‚

### 3.3 ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œ

```python
import time
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=60)
)
def generate_with_retry(client: OpenRouterClient, prompt: str) -> str:
    """ãƒªãƒˆãƒ©ã‚¤ä»˜ãAPIå‘¼ã³å‡ºã—"""
    return client.complete(prompt)

def batch_generate(
    client: OpenRouterClient,
    texts: list[str],
    prompt_template: str,
    delay: float = 0.5
) -> list[str]:
    """ãƒãƒƒãƒç”Ÿæˆï¼ˆãƒ¬ãƒ¼ãƒˆåˆ¶é™è€ƒæ…®ï¼‰"""
    results = []
    for text in texts:
        prompt = prompt_template.format(text=text)
        result = generate_with_retry(client, prompt)
        results.append(result)
        time.sleep(delay)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å›é¿
    return results
```

### 3.4 API ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã®è©³ç´°

#### Qwen3-235B-A22B ã®ãƒ¬ãƒ¼ãƒˆåˆ¶é™

OpenRouterçµŒç”±ã§Qwen3ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ:

| åˆ¶é™ç¨®åˆ¥ | å€¤ |
|---------|-----|
| åˆ†æ¬¡åˆ¶é™ | 20 RPMï¼ˆrequests per minuteï¼‰ |
| æ—¥æ¬¡åˆ¶é™ | ãªã—ï¼ˆæœ‰æ–™ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨æ™‚ï¼‰ |

æ¨å¥¨è¨­å®š: `max_concurrent: 10`, `request_delay: 0.3`

#### Anthropic API ã®ãƒ¬ãƒ¼ãƒˆåˆ¶é™ï¼ˆå‚è€ƒ: v1ã€œv3ã§ä½¿ç”¨ï¼‰

OpenRouterçµŒç”±ã§Claudeï¼ˆHaikuç­‰ï¼‰ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã€Anthropicã®ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãŒé©ç”¨ã•ã‚Œã‚‹ã€‚

| åˆ¶é™ç¨®åˆ¥ | èª¬æ˜ |
|---------|------|
| RPM | Requests per minuteï¼ˆ1åˆ†ã‚ãŸã‚Šã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ï¼‰ |
| ITPM | Input tokens per minuteï¼ˆ1åˆ†ã‚ãŸã‚Šã®å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼‰ |
| OTPM | Output tokens per minuteï¼ˆ1åˆ†ã‚ãŸã‚Šã®å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼‰ |

- **Tierãƒ™ãƒ¼ã‚¹**: åˆ©ç”¨é¡ã«å¿œã˜ã¦Tier 1ã€œ4ã«è‡ªå‹•æ˜‡æ ¼ã€åˆ¶é™ãŒç·©å’Œã•ã‚Œã‚‹
- **ãƒˆãƒ¼ã‚¯ãƒ³ãƒã‚±ãƒƒãƒˆ**: é€£ç¶šè£œå……å‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€‚å›ºå®šãƒªã‚»ãƒƒãƒˆã§ã¯ãªãå¾ã€…ã«å›å¾©
- **ãƒ¢ãƒ‡ãƒ«åˆ¥ç‹¬ç«‹**: ãƒ¢ãƒ‡ãƒ«ã”ã¨ã«åˆ¥ã€…ã®ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãŒé©ç”¨ã•ã‚Œã‚‹

#### OpenRouterçµŒç”±ã®å ´åˆ

- OpenRouterè‡ªä½“ã¯æœ‰æ–™ãƒ¢ãƒ‡ãƒ«ã«ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’è¨­ã‘ãªã„
- ãƒ—ãƒ­ãƒã‚¤ãƒ€ï¼ˆAnthropicï¼‰ã®ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãŒãã®ã¾ã¾é©ç”¨ã•ã‚Œã‚‹
- BYOKï¼ˆBring Your Own Keyï¼‰ã®å ´åˆã¯è‡ªèº«ã®Anthropicã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®åˆ¶é™ãŒé©ç”¨

#### 429ã‚¨ãƒ©ãƒ¼æ™‚ã®å¯¾å¿œ

ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ˜ãƒƒãƒ€ãƒ¼ã§åˆ¶é™çŠ¶æ³ã‚’ç¢ºèªå¯èƒ½:

| ãƒ˜ãƒƒãƒ€ãƒ¼ | èª¬æ˜ |
|---------|------|
| `retry-after` | å¾…æ©Ÿã™ã¹ãç§’æ•° |
| `anthropic-ratelimit-requests-remaining` | æ®‹ã‚Šãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•° |
| `anthropic-ratelimit-tokens-remaining` | æ®‹ã‚Šãƒˆãƒ¼ã‚¯ãƒ³æ•° |

å‚è€ƒ:

- [Anthropic Rate Limits](https://docs.anthropic.com/en/api/rate-limits)
- [OpenRouter Rate Limits](https://openrouter.ai/docs/api/reference/limits)

### 3.5 ä¸¦åˆ—ãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼ˆé«˜é€ŸåŒ–ï¼‰

é †æ¬¡å‡¦ç†ã§ã¯1ãƒªã‚¯ã‚¨ã‚¹ãƒˆ2-3ç§’ã‹ã‹ã‚‹å ´åˆã€1000ã‚µãƒ³ãƒ—ãƒ«Ã—2å›ã§ç´„80åˆ†ä»¥ä¸Šã‹ã‹ã‚‹ã€‚
ä¸¦åˆ—åŒ–ã«ã‚ˆã‚Šå¤§å¹…ãªé«˜é€ŸåŒ–ãŒå¯èƒ½ã€‚

#### å®Ÿè£…ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

```python
import asyncio
import httpx

class AsyncOpenRouterClient:
    def __init__(self, api_key: str, model: str, max_concurrent: int = 5):
        self.api_key = api_key
        self.model = model
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.client = httpx.AsyncClient(timeout=60.0)

    async def complete(self, prompt: str) -> str:
        async with self.semaphore:
            response = await self.client.post(
                "https://openrouter.ai/api/v1/chat/completions",
                headers={"Authorization": f"Bearer {self.api_key}"},
                json={
                    "model": self.model,
                    "messages": [{"role": "user", "content": prompt}]
                }
            )
            if response.status_code == 429:
                retry_after = int(response.headers.get("retry-after", 10))
                await asyncio.sleep(retry_after)
                return await self.complete(prompt)  # ãƒªãƒˆãƒ©ã‚¤
            response.raise_for_status()
            return response.json()["choices"][0]["message"]["content"]

async def batch_generate_async(
    client: AsyncOpenRouterClient,
    texts: list[str],
    prompt_template: str
) -> list[str]:
    tasks = [
        client.complete(prompt_template.format(text=t))
        for t in texts
    ]
    return await asyncio.gather(*tasks)
```

#### æ¨å¥¨è¨­å®š

| ä¸¦åˆ—åº¦ | ç”¨é€” |
|-------|------|
| 5 | æ§ãˆã‚ï¼ˆTier 1å‘ã‘ï¼‰ |
| 10 | ä¸­ç¨‹åº¦ï¼ˆTier 2-3å‘ã‘ï¼‰ |
| 20+ | é«˜è² è·ï¼ˆTier 4+ã€è¦ç¢ºèªï¼‰ |

429ã‚¨ãƒ©ãƒ¼ãŒé »ç™ºã™ã‚‹å ´åˆã¯ä¸¦åˆ—åº¦ã‚’ä¸‹ã’ã‚‹ã€‚

### 3.6 APIã‚³ã‚¹ãƒˆå®Ÿç¸¾

#### Qwen3-235B-A22Bï¼ˆv4ä»¥é™ã®è¦‹ç©ã‚‚ã‚Šï¼‰

| ã‚µãƒ³ãƒ—ãƒ«æ•° | æ¨å®šã‚³ã‚¹ãƒˆ |
|-----------|-----------|
| 1,000 | $0.23 |
| 5,000 | $1.17 |
| 10,000 | $2.34 |

#### Claude Haiku 4.5ï¼ˆv1ã€œv3ã®å®Ÿç¸¾ï¼‰

| é …ç›® | å€¤ |
|------|-----|
| ãƒ¢ãƒ‡ãƒ« | Claude Haiku 4.5 (via OpenRouter) |
| ã‚µãƒ³ãƒ—ãƒ«æ•° | 1,000 |
| ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•° | 2,000ï¼ˆSNSå¤‰æ› + çµµæ–‡å­—ç”Ÿæˆï¼‰ |
| ç·ã‚³ã‚¹ãƒˆ | $0.682 |
| 1ã‚µãƒ³ãƒ—ãƒ«ã‚ãŸã‚Š | ç´„ $0.00068 |
| 1ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚ãŸã‚Š | ç´„ $0.00034 |

**ã‚¹ã‚±ãƒ¼ãƒ«è¦‹ç©ã‚‚ã‚Šï¼ˆClaude Haiku 4.5ï¼‰:**

- 10,000ã‚µãƒ³ãƒ—ãƒ«: ç´„ $6.8
- 100,000ã‚µãƒ³ãƒ—ãƒ«: ç´„ $68

Qwen3-235B-A22Bã¯Claude Haiku 4.5ã®ç´„1/3ã®ã‚³ã‚¹ãƒˆã€‚

### 3.7 ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒãƒªã‚·ãƒ¼æ‹’å¦ã®æ¤œå‡º

OpenRouterçµŒç”±ã§LLMã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã€NSFWã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ¢ãƒ‡ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã«ã‚ˆã‚Šæ‹’å¦ã•ã‚Œã‚‹å ´åˆãŒã‚ã‚‹ã€‚
ã‚¨ãƒ©ãƒ¼ã‚’æ¤œå‡ºã—ã¦ãƒ­ã‚°ã«è¨˜éŒ²ã—ã€æ‹’å¦ç‡ã‚’ç›£è¦–ã™ã‚‹ã€‚

> **Note**: Claudeï¼ˆv1ã€œv3ï¼‰ã¯å³æ ¼ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ¢ãƒ‡ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æŒã¤ã€‚Qwen3ï¼ˆv4ä»¥é™ï¼‰ã¯æ¯”è¼ƒçš„ç·©ã„ãŒã€ãƒ—ãƒ­ãƒã‚¤ãƒ€ã«ã‚ˆã£ã¦ã¯åˆ¶é™ãŒã‚ã‚‹å ´åˆãŒã‚ã‚‹ã€‚

```python
import httpx

def is_content_policy_error(error: Exception) -> bool:
    """APIã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒãƒªã‚·ãƒ¼æ‹’å¦ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
    if isinstance(error, httpx.HTTPStatusError):
        # 403: ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ¢ãƒ‡ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é•å
        if error.response.status_code == 403:
            return True
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã«moderation/content/flagged/policyã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚‹ã‹ç¢ºèª
        try:
            body = error.response.text.lower()
            if any(kw in body for kw in ["moderation", "content", "flagged", "policy"]):
                return True
        except Exception:
            pass
    return False
```

**OpenRouterã®403ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹:**

```json
{
  "error": {
    "code": 403,
    "message": "Content moderation violation",
    "metadata": {
      "reasons": ["violence"],
      "flagged_input": "...",
      "provider_name": "anthropic"
    }
  }
}
```

## 4. çµµæ–‡å­—å‡¦ç†

### 4.1 çµµæ–‡å­—ãƒªã‚¹ãƒˆå–å¾—

```python
import emoji

def get_all_emojis() -> set[str]:
    """å…¨çµµæ–‡å­—ã®ã‚»ãƒƒãƒˆã‚’å–å¾—"""
    return set(emoji.EMOJI_DATA.keys())

# ç´„3,700å€‹ã®çµµæ–‡å­—
all_emojis = get_all_emojis()
```

### 4.2 è‚Œè‰²ãƒãƒªã‚¢ãƒ³ãƒˆæ­£è¦åŒ–

```python
import re

# è‚Œè‰²ä¿®é£¾å­ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
SKIN_TONE_PATTERN = re.compile(r'[\U0001F3FB-\U0001F3FF]')

def normalize_skin_tone(text: str) -> str:
    """è‚Œè‰²ãƒãƒªã‚¢ãƒ³ãƒˆã‚’åŸºæœ¬çµµæ–‡å­—ã«çµ±åˆ"""
    return SKIN_TONE_PATTERN.sub('', text)

# ä¾‹
normalize_skin_tone("ğŸ‘‹ğŸ»")  # â†’ "ğŸ‘‹"
normalize_skin_tone("ğŸ‘¨ğŸ½â€ğŸ’»")  # â†’ "ğŸ‘¨â€ğŸ’»"
```

### 4.3 çµµæ–‡å­—æŠ½å‡º

```python
def extract_emojis(text: str, max_count: int = 5) -> list[str]:
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰çµµæ–‡å­—ã‚’æŠ½å‡º"""
    # çµµæ–‡å­—ãƒªã‚¹ãƒˆã‚’å–å¾—
    emoji_list = emoji.emoji_list(text)

    # çµµæ–‡å­—ã®ã¿æŠ½å‡º
    emojis = [item['emoji'] for item in emoji_list]

    # è‚Œè‰²æ­£è¦åŒ–
    emojis = [normalize_skin_tone(e) for e in emojis]

    # æœ€å¤§æ•°ã§åˆ‡ã‚Šæ¨ã¦
    return emojis[:max_count]

# ä¾‹
extract_emojis("æ¥½ã—ã„ğŸ˜ŠğŸ‰âœ¨æœ€é«˜ï¼")  # â†’ ["ğŸ˜Š", "ğŸ‰", "âœ¨"]
```

## 5. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆ

### 5.1 ç”Ÿæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

```python
import json
from pathlib import Path
from dataclasses import dataclass
from tqdm import tqdm

@dataclass
class DataSample:
    original_text: str      # å…ƒã®Wikipediaæ–‡
    sns_text: str           # SNSé¢¨å¤‰æ›å¾Œ
    emojis: list[str]       # ç”Ÿæˆã•ã‚ŒãŸçµµæ–‡å­—
    emoji_string: str       # ç©ºç™½åŒºåˆ‡ã‚Šçµµæ–‡å­—åˆ—

def generate_dataset(
    client: OpenRouterClient,
    sentences: list[str],
    output_path: Path,
    batch_size: int = 100
) -> list[DataSample]:
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆ"""
    samples = []

    for i, sentence in enumerate(tqdm(sentences)):
        try:
            # SNSé¢¨å¤‰æ›
            sns_text = client.complete(
                SNS_CONVERSION_PROMPT.format(text=sentence)
            ).strip()

            # çµµæ–‡å­—ç”Ÿæˆ
            emoji_output = client.complete(
                EMOJI_GENERATION_PROMPT.format(text=sns_text)
            ).strip()

            # çµµæ–‡å­—æŠ½å‡ºãƒ»æ¤œè¨¼
            emojis = extract_emojis(emoji_output)
            if not emojis:
                continue  # çµµæ–‡å­—ãŒãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—

            sample = DataSample(
                original_text=sentence,
                sns_text=sns_text,
                emojis=emojis,
                emoji_string=" ".join(emojis)
            )
            samples.append(sample)

            # å®šæœŸä¿å­˜
            if (i + 1) % batch_size == 0:
                save_dataset(samples, output_path)

        except Exception as e:
            print(f"Error at {i}: {e}")
            continue

        time.sleep(0.5)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™

    save_dataset(samples, output_path)
    return samples

def save_dataset(samples: list[DataSample], path: Path):
    """JSONLå½¢å¼ã§ä¿å­˜"""
    with open(path, "w", encoding="utf-8") as f:
        for sample in samples:
            f.write(json.dumps(sample.__dict__, ensure_ascii=False) + "\n")
```

### 5.2 å“è³ªãƒã‚§ãƒƒã‚¯

```python
def validate_sample(sample: DataSample) -> bool:
    """ã‚µãƒ³ãƒ—ãƒ«ã®å“è³ªãƒã‚§ãƒƒã‚¯"""
    # çµµæ–‡å­—æ•°ãƒã‚§ãƒƒã‚¯
    if not (1 <= len(sample.emojis) <= 5):
        return False

    # SNSãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã§ãªã„ã‹
    if not sample.sns_text.strip():
        return False

    # çµµæ–‡å­—ä»¥å¤–ã®æ–‡å­—ãŒæ··å…¥ã—ã¦ã„ãªã„ã‹
    for e in sample.emojis:
        if not emoji.is_emoji(e):
            return False

    return True
```

## 6. T5ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

### 6.1 ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™

```python
from torch.utils.data import Dataset
from transformers import T5Tokenizer

class EmojiDataset(Dataset):
    def __init__(
        self,
        data_path: Path,
        tokenizer: T5Tokenizer,
        max_input_length: int = 128,
        max_output_length: int = 32
    ):
        self.samples = self._load_data(data_path)
        self.tokenizer = tokenizer
        self.max_input_length = max_input_length
        self.max_output_length = max_output_length

    def _load_data(self, path: Path) -> list[dict]:
        samples = []
        with open(path, "r", encoding="utf-8") as f:
            for line in f:
                samples.append(json.loads(line))
        return samples

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        sample = self.samples[idx]

        # å…¥åŠ›: SNSãƒ†ã‚­ã‚¹ãƒˆ
        input_text = sample["sns_text"]

        # å‡ºåŠ›: çµµæ–‡å­—åˆ—
        output_text = sample["emoji_string"]

        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
        input_encoding = self.tokenizer(
            input_text,
            max_length=self.max_input_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )

        output_encoding = self.tokenizer(
            output_text,
            max_length=self.max_output_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )

        return {
            "input_ids": input_encoding["input_ids"].squeeze(),
            "attention_mask": input_encoding["attention_mask"].squeeze(),
            "labels": output_encoding["input_ids"].squeeze()
        }
```

### 6.2 çµµæ–‡å­—ãƒˆãƒ¼ã‚¯ãƒ³è¿½åŠ 

```python
from transformers import T5Tokenizer, T5ForConditionalGeneration

def setup_model_with_emoji_tokens(model_name: str = "sonoisa/t5-base-japanese"):
    """çµµæ–‡å­—ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’æº–å‚™"""
    tokenizer = T5Tokenizer.from_pretrained(model_name, legacy=False)
    model = T5ForConditionalGeneration.from_pretrained(model_name)

    # çµµæ–‡å­—ã‚’ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã¨ã—ã¦è¿½åŠ 
    emoji_tokens = list(get_all_emojis())
    num_added = tokenizer.add_tokens(emoji_tokens)
    print(f"Added {num_added} emoji tokens")

    # åŸ‹ã‚è¾¼ã¿å±¤ã‚’ãƒªã‚µã‚¤ã‚º
    model.resize_token_embeddings(len(tokenizer))

    return tokenizer, model
```

### 6.3 å­¦ç¿’ãƒ«ãƒ¼ãƒ—

```python
from transformers import Trainer, TrainingArguments

def train_model(
    model,
    tokenizer,
    train_dataset,
    eval_dataset,
    output_dir: str = "outputs/models"
):
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=10,
        per_device_train_batch_size=16,
        per_device_eval_batch_size=16,
        learning_rate=1e-3,
        weight_decay=0.01,
        evaluation_strategy="epoch",
        save_strategy="epoch",
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        logging_steps=100,
        warmup_steps=500,
        fp16=True,  # A100ã§ã¯æœ‰åŠ¹
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
    )

    trainer.train()
    return trainer
```

## 7. æ¨è«–

```python
def translate_to_emoji(
    model,
    tokenizer,
    text: str,
    max_length: int = 32,
    num_beams: int = 4
) -> str:
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰çµµæ–‡å­—ã‚’ç”Ÿæˆ"""
    inputs = tokenizer(
        text,
        return_tensors="pt",
        max_length=128,
        truncation=True
    ).to(model.device)

    outputs = model.generate(
        **inputs,
        max_length=max_length,
        num_beams=num_beams,
        early_stopping=True
    )

    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return result

# ä½¿ç”¨ä¾‹
emojis = translate_to_emoji(model, tokenizer, "ä»Šæ—¥ã¯æ¥½ã—ã‹ã£ãŸ")
print(emojis)  # ğŸ˜Š ğŸ‰
```

## 8. ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### OOMã‚¨ãƒ©ãƒ¼ï¼ˆGPU ãƒ¡ãƒ¢ãƒªä¸è¶³ï¼‰

```python
# ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å°ã•ãã™ã‚‹
training_args.per_device_train_batch_size = 8

# å‹¾é…ç´¯ç©ã‚’ä½¿ã†
training_args.gradient_accumulation_steps = 2

# FP16ã‚’æœ‰åŠ¹ã«ã™ã‚‹ï¼ˆA100ã§ã¯æ¨™æº–ã§æœ‰åŠ¹ï¼‰
training_args.fp16 = True
```

### çµµæ–‡å­—ãŒOOVã«ãªã‚‹

```python
# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã«çµµæ–‡å­—ã‚’è¿½åŠ ã—ãŸã‹ç¢ºèª
print(tokenizer.encode("ğŸ˜Š"))  # [çµµæ–‡å­—ã®ID, </s>]

# è¿½åŠ ã•ã‚Œã¦ã„ãªã„å ´åˆã¯å†åº¦è¿½åŠ 
tokenizer.add_tokens(["ğŸ˜Š", "ğŸ‰", ...])
model.resize_token_embeddings(len(tokenizer))
```

### APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼

```python
# ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’å»¶é•·
client = httpx.Client(timeout=120.0)

# ãƒªãƒˆãƒ©ã‚¤è¨­å®šã‚’èª¿æ•´
@retry(stop=stop_after_attempt(5), wait=wait_exponential(max=120))
def call_api(...):
    ...
```
