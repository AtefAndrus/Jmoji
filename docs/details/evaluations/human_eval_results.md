# 人手評価結果

## 概要

本ドキュメントは、Jmojiプロジェクトの絵文字予測モデルに対する人手評価の結果を記録する。

**実施日**: 2026-01-08
**評価対象**: 20サンプル
**評価者数**: 1名

---

## 評価対象モデル

| モデル | Jaccard | 多様性 | 説明 |
|--------|---------|--------|------|
| v4_focal_top50（モデルA） | 0.182 | 14% | Focal Loss適用、精度最良 |
| v4_top50（モデルB） | 0.165 | 21% | 標準学習、バランス型 |
| 教師モデル（Gold） | - | - | Qwen3-235B-A22B生成 |

---

## 評価基準

### 評価項目

| 項目 | スケール | 説明 |
|------|----------|------|
| 意味的一致度 | 0-4 | テキストの意味・感情を絵文字が表現しているか |
| 自然さ | 0-4 | 日本のSNSで見かけそうな使い方か |
| 誤解の可能性 | Yes/No | 絵文字が元の文の意図と逆の印象を与えそうか |
| モデル選好 | A/B/同等 | どちらのモデル出力が良いか |

---

## 評価結果

### 平均スコア

| モデル | 意味的一致度 | 自然さ | 誤解率 |
|--------|-------------|--------|--------|
| 教師（Gold） | 2.30 ± 0.56 | 2.15 ± 0.36 | 0.0% |
| モデルA（focal_top50） | 1.00 ± 0.71 | 1.30 ± 0.56 | 10.0% |
| モデルB（top50） | 0.90 ± 0.44 | 1.25 ± 0.43 | 5.0% |

### スコア解釈

- **教師モデル**: 意味的一致度2.30/4.0は「普通〜概ね妥当」レベル
- **学生モデル**: 両モデルとも1.0前後で「部分的に関連〜一応意味は通る」レベル
- **教師vs学生のギャップ**: 約1.3ポイント（教師が優位）

### モデル選好

| 選好 | 票数 | 割合 |
|------|------|------|
| モデルA（focal_top50） | 6 | 30.0% |
| モデルB（top50） | 3 | 15.0% |
| 同等 | 11 | 55.0% |

### 選好分析

- 過半数（55%）が「同等」と判断
- focal_top50がやや優位（6票 vs 3票）
- LLM評価では逆にtop50が優位だった（9勝6敗）

---

## LLM評価との比較

| 評価方法 | focal_top50 | top50 | 優位モデル |
|----------|-------------|-------|-----------|
| 人手評価（選好） | 6票 | 3票 | focal_top50 |
| LLM評価（勝敗） | 6勝 | 9勝 | top50 |
| 人手評価（意味的一致度） | 1.00 | 0.90 | focal_top50 |
| LLM評価（意味的一致度） | 1.55 | 1.75 | top50 |

### 考察

1. **人手評価とLLM評価で逆転**
   - 人手評価: focal_top50がやや優位
   - LLM評価: top50が優位

2. **評価サンプルの違い**
   - 人手評価: 20件（テストセットからランダム抽出）
   - LLM評価: 20件（別途抽出、同一サンプルではない）
   - サンプル特性の違いが結果に影響した可能性あり

3. **評価基準の違い**
   - **LLM評価**: 過剰生成（😊😊😊のような繰り返し）に厳格。自然さスコアで大きく減点
   - **人手評価**: 絵文字の意味的関連性を重視。繰り返しに対してLLMほど厳しくない傾向
   - LLMはrepetition penaltyなしの出力を「不自然」と評価しやすい

4. **55%が「同等」と判断した理由**
   - 両モデルの出力品質が近接している（Jaccard差0.017）
   - 学生モデル同士の比較では差が小さく、どちらも教師モデルに対して改善の余地がある
   - 評価者が「どちらも低品質で差がつかない」と判断した可能性

5. **結論**
   - 両モデルの品質差は小さく、統計的に有意な差を検出するには現在のサンプル数では不十分
   - LLM評価と人手評価の乖離は、評価基準の違いに起因する可能性が高い
   - 用途に応じた選択が適切（自然さ重視ならrepetition penalty適用を推奨）

---

## 主要な発見

### 1. 教師モデルと学生モデルのギャップ

| 指標 | 教師 | 学生（平均） | ギャップ |
|------|------|-------------|---------|
| 意味的一致度 | 2.30 | 0.95 | -1.35 |
| 自然さ | 2.15 | 1.28 | -0.87 |

**解釈**: 学生モデルは教師モデルに対して約60%の品質。改善の余地あり。

### 2. 誤解率

- 教師モデル: 0%
- focal_top50: 10%（2/20件）
- top50: 5%（1/20件）

**解釈**: 学生モデルで稀に誤解を招く出力が発生。focal_top50の方がやや高リスク。

### 3. 「同等」判断の多さ

- 55%のサンプルで「どちらとも言えない」
- 両モデルの出力品質が近接していることを示唆
- または、両モデルとも低品質で差がつかない可能性

---

## 課題と改善案

| 課題 | 詳細 | 改善案 |
|------|------|--------|
| 教師とのギャップ大 | 意味的一致度1.35ポイント差 | より大規模なデータセットでの学習 |
| 評価者数不足 | 1名のみで評価者間一致度計算不可 | 追加評価者の確保 |
| サンプル数 | 20件は統計的信頼性に限界 | 50〜100件への拡大 |

---

## 評価者間一致度

現時点では評価者1名のため、Cohen's kappa等の評価者間一致度は計算不可。

追加評価者を確保した場合に計算予定。

---

## 成果物

| ファイル | 内容 |
|----------|------|
| `outputs/human_eval/samples.jsonl` | 評価サンプル（20件） |
| `outputs/human_eval/results.json` | 集計結果（JSON） |
| `outputs/human_eval/report.md` | 自動生成レポート |
| `scripts/analyze_human_eval.py` | 結果集計スクリプト |

---

## 関連ドキュメント

- [llm_eval_results.md](llm_eval_results.md): LLM-as-a-Judge評価結果
- [v4_results.md](../experiments/v4_results.md): v4データセット学習実験結果
- [evaluation.md](../evaluation.md): 評価方法の詳細
