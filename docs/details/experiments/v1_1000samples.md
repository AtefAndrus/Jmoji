# 実験記録: データセットv1（1,000件）での学習結果

## 概要

本ドキュメントは、1,000件の疑似対訳データセット（dataset_v1.jsonl）を用いたT5モデル学習の実験結果を記録する。

**結論**: データセットの絵文字分布の偏り（✨が18.6%）により、mode collapseが発生。学習アルゴリズムの調整では解決できず、データセットの再生成が必要と判断した。

---

## 1. データセット統計

### 基本情報

| 項目 | 値 |
|------|-----|
| サンプル数 | 1,000 |
| 絵文字総出現数 | 2,682 |
| ユニーク絵文字数 | 296 |
| 平均絵文字数/サンプル | 2.68 |

### 絵文字頻度分布（Top 20）

| 順位 | 絵文字 | 出現数 | 割合 |
|------|--------|--------|------|
| 1 | ✨ | 500 | 18.6% |
| 2 | 📚 | 111 | 4.1% |
| 3 | 🎉 | 95 | 3.5% |
| 4 | 😅 | 86 | 3.2% |
| 5 | ⚽ | 73 | 2.7% |
| 6 | 🤔 | 72 | 2.7% |
| 7 | 🌞 | 65 | 2.4% |
| 8 | 🔬 | 59 | 2.2% |
| 9 | 🎵 | 55 | 2.1% |
| 10 | 🔥 | 43 | 1.6% |
| 11 | 💔 | 42 | 1.6% |
| 12 | 🏀 | 42 | 1.6% |
| 13 | 😄 | 38 | 1.4% |
| 14 | 😊 | 37 | 1.4% |
| 15 | 😢 | 37 | 1.4% |
| 16 | 💭 | 32 | 1.2% |
| 17 | 🌍 | 29 | 1.1% |
| 18 | 🏆 | 28 | 1.0% |
| 19 | 😲 | 28 | 1.0% |
| 20 | 🎮 | 26 | 1.0% |

**問題点**: ✨が1位（18.6%）で、2位の📚（4.1%）の約4.5倍。この極端な偏りがmode collapseの原因となった。

---

## 2. 学習実験

### 実験環境

- **GPU**: Google Colab A100 80GB
- **モデル**: sonoisa/t5-base-japanese（222Mパラメータ）
- **追加トークン**: 5,222個（絵文字）
- **データ分割**: Train 80% / Val 10% / Test 10%

### 試行1: 初期設定

| パラメータ | 値 |
|------------|-----|
| epochs | 10 |
| learning_rate | 0.001 |
| warmup_steps | 500 |
| batch_size | 16 |
| fp16 | True |

**結果**:

- `eval_loss`: NaN
- `train_loss`: 0.0
- `grad_norm`: NaN

**原因**: ラベルのパディングトークンを-100に置き換えていなかったため、損失計算が正しく行われなかった。

### 試行2: パディング修正後

EmojiDatasetで`labels[labels == pad_token_id] = -100`を追加。

**結果**: 依然としてNaN

**原因**: fp16（混合精度）と新規追加された絵文字トークンの埋め込み初期化の組み合わせで、勾配がオーバーフロー。

### 試行3: fp16無効化、学習率調整

| パラメータ | 値 |
|------------|-----|
| epochs | 10 |
| learning_rate | 1e-4 |
| warmup_steps | 50 |
| fp16 | False |

**結果**:

- `eval_loss`: 8.0
- 学習は進行するが、出力が「填」「よね」など絵文字ではない文字列

**原因**: 学習が不十分。エポック数・学習率の調整が必要。

### 試行4: エポック増加

| パラメータ | 値 |
|------------|-----|
| epochs | 30 |
| learning_rate | 5e-4 |
| warmup_steps | 100 |
| fp16 | False |

**結果**:

- `eval_loss`: 4.17

**推論テスト**:

```text
入力: ママがプロのバイオリニストだったから...
期待: 🎻 🎹 🎵
出力: ✨ ✨ ✨

入力: 今日は楽しかった
出力: ✨ ✨
```

**評価指標**:

| 指標 | 値 |
|------|-----|
| Average Jaccard | 0.177 |
| Exact Match Rate | 0.0% |

**問題**: mode collapse発生。モデルがほぼ全ての入力に対して✨のみを出力。

### 試行5: label_smoothing追加

| パラメータ | 値 |
|------------|-----|
| epochs | 50 |
| learning_rate | 3e-4 |
| warmup_steps | 150 |
| fp16 | False |
| label_smoothing | 0.1 |
| data_collator | DataCollatorForSeq2Seq |

**結果**:

- `eval_loss`: 5.68

**推論テスト**:

```text
入力: ママがプロのバイオリニストだったから...
期待: 🎻 🎹 🎵
出力: ✨ ✨ ✨

入力: 今日は楽しかった
出力: ✨ ✨ ✨

入力: 明日は雨らしい
出力: ✨ 🤔 ✨
```

**評価指標**:

| 指標 | 値 |
|------|-----|
| Average Jaccard | 0.1707 |
| Exact Match Rate | 0.0% |

**結論**: label_smoothingを追加してもmode collapseは解消されず。

---

## 3. Temperature Sampling の効果

Beam search（決定的）とTemperature sampling（確率的）の比較:

```python
# Beam search
outputs = model.generate(**inputs, num_beams=4, early_stopping=True)

# Temperature sampling
outputs = model.generate(**inputs, do_sample=True, temperature=1.0, top_k=50, top_p=0.95)
```

**結果**:

```text
入力: ママがプロのバイオリニストだったから...
  Beam: ✨ ✨ ✨
  Sample: ✨ 📚 🎉

入力: クーパーは最初はアワーバックのやり方を...
  Beam: ✨ ✨
  Sample: ✨ 🎉
```

Temperature samplingにより若干の多様性は出るが、根本的な解決には至らず。

---

## 4. 分析と結論

### Mode Collapse の原因

1. **データの偏り**: ✨が18.6%を占め、他の絵文字の4倍以上
2. **クロスエントロピー損失の性質**: 頻出クラスを出力すれば損失が下がりやすい
3. **データ量不足**: 296種類の絵文字に対して1,000サンプルでは、各絵文字の平均出現回数が約3.4回

### 試した対策と効果

| 対策 | 効果 |
|------|------|
| label_smoothing | 効果なし |
| 学習率調整 | 効果なし |
| エポック数増加 | 効果なし |
| Temperature sampling | 若干の多様性向上だが不十分 |

### 必要な対策

1. **プロンプト改善**: ✨の使用を禁止
2. **データ量増加**: 1,000件 → 5,000件以上
3. **絵文字バランス**: 特定絵文字への偏りを監視

---

## 5. 次のステップ

dataset_v2（5,000件）の生成に移行。以下の改善を適用:

1. プロンプトに「✨は使用しないでください」を追加
2. 具体的な内容に関連する絵文字を優先するよう指示
3. 生成後に絵文字分布を確認し、偏りがないことを検証

---

## 付録: 生成コスト

| 項目 | 値 |
|------|-----|
| モデル | Claude Haiku 4.5 (via OpenRouter) |
| サンプル数 | 1,000 |
| リクエスト数 | 2,000（SNS変換 + 絵文字生成） |
| 総コスト | $0.68 |
| 1サンプルあたり | 約 $0.00068 |
