# 人手評価結果

## 概要

本ドキュメントは、Jmojiプロジェクトの絵文字予測モデルに対する人手評価の結果を記録する。

**実施日**: 2026-01-08
**評価対象**: 20サンプル
**評価者数**: 1名

---

## 評価対象モデル

| モデル | Jaccard | 多様性 | 説明 |
|--------|---------|--------|------|
| v4_focal_top50（モデルA） | 0.182 | 14% | Focal Loss適用、精度最良 |
| v4_top50（モデルB） | 0.165 | 21% | 標準学習、バランス型 |
| 教師モデル（Gold） | - | - | Qwen3-235B-A22B生成 |

---

## 評価基準

### 評価項目

| 項目 | スケール | 説明 |
|------|----------|------|
| 意味的一致度 | 0-4 | テキストの意味・感情を絵文字が表現しているか |
| 自然さ | 0-4 | 日本のSNSで見かけそうな使い方か |
| 誤解の可能性 | Yes/No | 絵文字が元の文の意図と逆の印象を与えそうか |
| モデル選好 | A/B/同等 | どちらのモデル出力が良いか |

---

## 評価結果

### 平均スコア

| モデル | 意味的一致度 | 自然さ | 誤解率 |
|--------|-------------|--------|--------|
| 教師（Gold） | 2.30 ± 0.56 | 2.15 ± 0.36 | 0.0% |
| モデルA（focal_top50） | 1.00 ± 0.71 | 1.30 ± 0.56 | 10.0% |
| モデルB（top50） | 0.90 ± 0.44 | 1.25 ± 0.43 | 5.0% |

### スコア解釈

- **教師モデル**: 意味的一致度2.30/4.0は「普通〜概ね妥当」レベル
- **学生モデル**: 両モデルとも1.0前後で「部分的に関連〜一応意味は通る」レベル
- **教師vs学生のギャップ**: 約1.3ポイント（教師が優位）

### モデル選好

| 選好 | 票数 | 割合 |
|------|------|------|
| モデルA（focal_top50） | 6 | 30.0% |
| モデルB（top50） | 3 | 15.0% |
| 同等 | 11 | 55.0% |

### 選好分析

- 過半数（55%）が「同等」と判断
- focal_top50がやや優位（6票 vs 3票）
- LLM評価では逆にtop50が優位だった（9勝6敗）

---

## LLM評価との比較

| 評価方法 | focal_top50 | top50 | 優位モデル |
|----------|-------------|-------|-----------|
| 人手評価（選好） | 6票 | 3票 | focal_top50 |
| LLM評価（勝敗） | 6勝 | 9勝 | top50 |
| 人手評価（意味的一致度） | 1.00 | 0.90 | focal_top50 |
| LLM評価（意味的一致度） | 1.55 | 1.75 | top50 |

### 考察

1. **人手評価とLLM評価で逆転**
   - 人手評価: focal_top50がやや優位
   - LLM評価: top50が優位

2. **可能な原因**
   - 評価サンプル数の違い（人手20件 vs LLM20件、同一サンプルではない可能性）
   - 評価者の基準の違い（人間1名 vs LLM）
   - LLMが過剰生成に厳しい評価をした可能性

3. **結論**
   - 両モデルの品質差は小さい（55%が同等）
   - 用途に応じた選択が適切

---

## 主要な発見

### 1. 教師モデルと学生モデルのギャップ

| 指標 | 教師 | 学生（平均） | ギャップ |
|------|------|-------------|---------|
| 意味的一致度 | 2.30 | 0.95 | -1.35 |
| 自然さ | 2.15 | 1.28 | -0.87 |

**解釈**: 学生モデルは教師モデルに対して約60%の品質。改善の余地あり。

### 2. 誤解率

- 教師モデル: 0%
- focal_top50: 10%（2/20件）
- top50: 5%（1/20件）

**解釈**: 学生モデルで稀に誤解を招く出力が発生。focal_top50の方がやや高リスク。

### 3. 「同等」判断の多さ

- 55%のサンプルで「どちらとも言えない」
- 両モデルの出力品質が近接していることを示唆
- または、両モデルとも低品質で差がつかない可能性

---

## 課題と改善案

| 課題 | 詳細 | 改善案 |
|------|------|--------|
| 教師とのギャップ大 | 意味的一致度1.35ポイント差 | より大規模なデータセットでの学習 |
| 評価者数不足 | 1名のみで評価者間一致度計算不可 | 追加評価者の確保 |
| サンプル数 | 20件は統計的信頼性に限界 | 50〜100件への拡大 |

---

## 評価者間一致度

現時点では評価者1名のため、Cohen's kappa等の評価者間一致度は計算不可。

追加評価者を確保した場合に計算予定。

---

## 成果物

| ファイル | 内容 |
|----------|------|
| `outputs/human_eval/samples.jsonl` | 評価サンプル（20件） |
| `outputs/human_eval/results.json` | 集計結果（JSON） |
| `outputs/human_eval/report.md` | 自動生成レポート |
| `scripts/analyze_human_eval.py` | 結果集計スクリプト |

---

## 関連ドキュメント

- [llm_eval_results.md](llm_eval_results.md): LLM-as-a-Judge評価結果
- [experiment_v4_results.md](experiment_v4_results.md): v4データセット学習実験結果
- [evaluation.md](../evaluation.md): 評価方法の詳細
