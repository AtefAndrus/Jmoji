# %% [markdown]
# # Jmoji T5 Training on Google Colab
#
# æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆâ†’çµµæ–‡å­—ç¿»è¨³ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯

# %% [markdown]
# ## 1. ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

# %%
# Google Driveã‚’ãƒã‚¦ãƒ³ãƒˆ
from google.colab import drive
drive.mount('/content/drive')

# %%
# ãƒªãƒã‚¸ãƒˆãƒªã‚¯ãƒ­ãƒ¼ãƒ³
!git clone https://github.com/AtefAndrus/Jmoji.git
%cd /content/Jmoji

# %%
# ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆpyproject.tomlã‹ã‚‰è‡ªå‹•è§£æ±ºï¼‰
!pip install -q .

# %%
# GPUç¢ºèª
import torch
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"Device: {torch.cuda.get_device_name(0)}")
    print(f"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

# %% [markdown]
# ## 2. è¨­å®š

# %%
# ãƒ‘ã‚¹è¨­å®š
DATA_PATH = "/content/drive/MyDrive/school/ai_application/dataset_v3.jsonl"
OUTPUT_DIR = "/content/Jmoji/outputs/models"
EVAL_DIR = "/content/Jmoji/outputs/evaluation"

# å­¦ç¿’è¨­å®š
CONFIG = {
    "model_name": "sonoisa/t5-base-japanese",
    "num_epochs": 50,
    "batch_size": 16,
    "learning_rate": 3e-4,
    "weight_decay": 0.01,
    "warmup_steps": 150,
    "max_input_length": 128,
    "max_output_length": 32,
    "train_ratio": 0.8,
    "val_ratio": 0.1,
    "test_ratio": 0.1,
    "fp16": False,  # NaNé˜²æ­¢ã®ãŸã‚ã‚ªãƒ•
    "logging_steps": 50,
    "label_smoothing": 0.1,  # mode collapseå¯¾ç­–
    "early_stopping_patience": 5,
}

print("Config:")
for k, v in CONFIG.items():
    print(f"  {k}: {v}")

# %% [markdown]
# ## 3. ãƒ‡ãƒ¼ã‚¿æº–å‚™

# %%
import sys
sys.path.append("/content/Jmoji")

from src.models.t5_trainer import (
    EmojiDataset,
    TrainConfig,
    setup_model_with_emoji_tokens,
    build_trainer,
    split_dataset,
    load_jsonl,
    generate_emoji,
    evaluate_model,
)

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
samples = load_jsonl(DATA_PATH)
print(f"Total samples: {len(samples)}")

# åˆ†å‰²
train_samples, val_samples, test_samples = split_dataset(
    samples,
    CONFIG["train_ratio"],
    CONFIG["val_ratio"]
)
print(f"Train: {len(train_samples)}, Val: {len(val_samples)}, Test: {len(test_samples)}")

# ã‚µãƒ³ãƒ—ãƒ«ç¢ºèª
print("\nSample data:")
for i, s in enumerate(train_samples[:3]):
    print(f"  [{i}] {s['sns_text'][:50]}... -> {s['emoji_string']}")

# %% [markdown]
# ## 3.5 çµµæ–‡å­—åˆ†å¸ƒã®ç¢ºèª

# %%
from collections import Counter

# å…¨ã‚µãƒ³ãƒ—ãƒ«ã®çµµæ–‡å­—ã‚’é›†è¨ˆ
all_emojis = []
for sample in samples:
    emojis = sample["emoji_string"].split()
    all_emojis.extend(emojis)

# é »åº¦ã‚«ã‚¦ãƒ³ãƒˆ
emoji_counts = Counter(all_emojis)
print(f"Total emoji occurrences: {len(all_emojis)}")
print(f"Unique emojis: {len(emoji_counts)}")
print("\nTop 20 emojis:")
for emoji, count in emoji_counts.most_common(20):
    pct = count / len(all_emojis) * 100
    print(f"  {emoji}: {count} ({pct:.1f}%)")

# æœ€é »å‡ºçµµæ–‡å­—ã®å‰²åˆã‚’è­¦å‘Š
top_emoji, top_count = emoji_counts.most_common(1)[0]
top_pct = top_count / len(all_emojis) * 100
if top_pct > 15:
    print(f"\nWarning: '{top_emoji}' is {top_pct:.1f}% of all emojis. This may cause mode collapse.")

# %% [markdown]
# ## 4. ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶æº–å‚™

# %%
tokenizer, model = setup_model_with_emoji_tokens(CONFIG["model_name"])
print(f"Vocab size: {len(tokenizer)}")

# çµµæ–‡å­—ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ç¢ºèª
test_emoji = "ğŸ˜Š ğŸ‰"
ids = tokenizer.encode(test_emoji, add_special_tokens=False)
decoded = tokenizer.decode(ids)
print(f"Emoji tokenization test: '{test_emoji}' -> {ids} -> '{decoded}'")

# %% [markdown]
# ## 5. Datasetæº–å‚™

# %%
# Datasetä½œæˆ
train_dataset = EmojiDataset(
    train_samples, tokenizer,
    CONFIG["max_input_length"],
    CONFIG["max_output_length"]
)
val_dataset = EmojiDataset(
    val_samples, tokenizer,
    CONFIG["max_input_length"],
    CONFIG["max_output_length"]
)
test_dataset = EmojiDataset(
    test_samples, tokenizer,
    CONFIG["max_input_length"],
    CONFIG["max_output_length"]
)

print(f"Train dataset: {len(train_dataset)}")
print(f"Val dataset: {len(val_dataset)}")
print(f"Test dataset: {len(test_dataset)}")

# ãƒ‡ãƒ¼ã‚¿ç¢ºèª
item = train_dataset[0]
print(f"\nSample item shapes:")
print(f"  input_ids: {item['input_ids'].shape}")
print(f"  attention_mask: {item['attention_mask'].shape}")
print(f"  labels: {item['labels'].shape}")
print(f"  Non -100 labels: {(item['labels'] != -100).sum().item()}")

# %% [markdown]
# ## 6. å­¦ç¿’

# %%
import os

# å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(EVAL_DIR, exist_ok=True)

# TrainConfigã‚’æ§‹ç¯‰
train_config = TrainConfig(
    model_name=CONFIG["model_name"],
    output_dir=OUTPUT_DIR,
    num_train_epochs=CONFIG["num_epochs"],
    per_device_train_batch_size=CONFIG["batch_size"],
    per_device_eval_batch_size=CONFIG["batch_size"],
    learning_rate=CONFIG["learning_rate"],
    weight_decay=CONFIG["weight_decay"],
    warmup_steps=CONFIG["warmup_steps"],
    logging_steps=CONFIG["logging_steps"],
    fp16=CONFIG["fp16"],
    label_smoothing_factor=CONFIG["label_smoothing"],
    early_stopping_patience=CONFIG["early_stopping_patience"],
    save_total_limit=3,
)

# Traineræ§‹ç¯‰
trainer = build_trainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    cfg=train_config,
)

# GPUç§»å‹•
if torch.cuda.is_available():
    model.to("cuda")

print("Starting training...")

# %%
# å­¦ç¿’å®Ÿè¡Œ
trainer.train()

# %%
# ãƒ¢ãƒ‡ãƒ«ä¿å­˜
trainer.save_model(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)
print(f"Model saved to {OUTPUT_DIR}")

# è©•ä¾¡çµæœä¿å­˜
eval_result = trainer.evaluate()
print(f"\nEval results: {eval_result}")

with open(f"{EVAL_DIR}/train_eval_results.txt", "w") as f:
    f.write(str(eval_result))

# %% [markdown]
# ## 7. æ¨è«–ãƒ†ã‚¹ãƒˆ

# %%
# å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã§ãƒ†ã‚¹ãƒˆï¼ˆæš—è¨˜ç¢ºèªï¼‰
print("=== å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã§ã®ãƒ†ã‚¹ãƒˆï¼ˆSamplingï¼‰ ===")
for sample in train_samples[:5]:
    text = sample["sns_text"]
    expected = sample["emoji_string"]
    result = generate_emoji(model, tokenizer, text, use_sampling=True)
    match = "OK" if result.strip() == expected.strip() else "NG"
    print(f"[{match}] å…¥åŠ›: {text[:40]}...")
    print(f"     æœŸå¾…: {expected}")
    print(f"     å‡ºåŠ›: {result}")
    print()

# %%
# Beam search ã¨ã®æ¯”è¼ƒ
print("=== Beam Search vs Sampling æ¯”è¼ƒ ===")
for sample in train_samples[:3]:
    text = sample["sns_text"]
    expected = sample["emoji_string"]
    result_beam = generate_emoji(model, tokenizer, text, use_sampling=False)
    result_sample = generate_emoji(model, tokenizer, text, use_sampling=True)
    print(f"å…¥åŠ›: {text[:40]}...")
    print(f"  æœŸå¾…: {expected}")
    print(f"  Beam: {result_beam}")
    print(f"  Sample: {result_sample}")
    print()

# %%
# æ–°è¦ãƒ†ã‚­ã‚¹ãƒˆã§ãƒ†ã‚¹ãƒˆ
print("=== æ–°è¦ãƒ†ã‚­ã‚¹ãƒˆã§ã®ãƒ†ã‚¹ãƒˆ ===")
test_texts = [
    "ä»Šæ—¥ã¯æ¥½ã—ã‹ã£ãŸ",
    "æ˜æ—¥ã¯é›¨ã‚‰ã—ã„",
    "ãŠãªã‹ã™ã„ãŸ",
    "è©¦é¨“ã«åˆæ ¼ã—ãŸ",
    "æ¨ã—ãŒå°Šã„",
    "ã‚ã£ã¡ã‚ƒçœ ã„",
]

for text in test_texts:
    result = generate_emoji(model, tokenizer, text, use_sampling=True)
    print(f"å…¥åŠ›: {text}")
    print(f"å‡ºåŠ›: {result}")
    print()

# %% [markdown]
# ## 8. è©•ä¾¡æŒ‡æ¨™

# %%
# ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§è©•ä¾¡ï¼ˆå…¨ä»¶ï¼‰
print("=== ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆè©•ä¾¡ ===")
eval_results = evaluate_model(model, tokenizer, test_samples)
print(f"Average Jaccard: {eval_results.avg_jaccard:.4f}")
print(f"Exact Match Rate: {eval_results.exact_match_rate:.4f}")
print(f"Micro F1: {eval_results.micro_f1:.4f}")
print(f"Avg Precision: {eval_results.avg_precision:.4f}")
print(f"Avg Recall: {eval_results.avg_recall:.4f}")
print(f"Avg F1: {eval_results.avg_f1:.4f}")
print(f"Samples evaluated: {eval_results.num_samples}")

# %%
# çµæœä¿å­˜
import json
with open(f"{EVAL_DIR}/test_metrics.json", "w", encoding="utf-8") as f:
    json.dump({
        "avg_jaccard": eval_results.avg_jaccard,
        "exact_match_rate": eval_results.exact_match_rate,
        "micro_f1": eval_results.micro_f1,
        "avg_precision": eval_results.avg_precision,
        "avg_recall": eval_results.avg_recall,
        "avg_f1": eval_results.avg_f1,
        "num_samples": eval_results.num_samples,
    }, f, ensure_ascii=False, indent=2)

print(f"Metrics saved to {EVAL_DIR}/test_metrics.json")
