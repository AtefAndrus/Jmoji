{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0259f31e",
   "metadata": {},
   "source": [
    "# Jmoji T5 Training on Google Colab\n",
    "\n",
    "æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆâ†’çµµæ–‡å­—ç¿»è¨³ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bcf1eb",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd6e4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Driveã‚’ãƒã‚¦ãƒ³ãƒˆ\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aada32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒªãƒã‚¸ãƒˆãƒªã‚¯ãƒ­ãƒ¼ãƒ³\n",
    "!git clone https://github.com/AtefAndrus/Jmoji.git\n",
    "%cd /content/Jmoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357bb5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "!pip install -r requirements-colab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3044d4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPUç¢ºèª\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1668c57d",
   "metadata": {},
   "source": [
    "## 2. è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e78e4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‘ã‚¹è¨­å®š\n",
    "DATA_PATH = \"/content/drive/MyDrive/school/ai_application/dataset_v1.jsonl\"\n",
    "OUTPUT_DIR = \"/content/Jmoji/outputs/models\"\n",
    "EVAL_DIR = \"/content/Jmoji/outputs/evaluation\"\n",
    "\n",
    "# å­¦ç¿’è¨­å®š\n",
    "CONFIG = {\n",
    "    \"model_name\": \"sonoisa/t5-base-japanese\",\n",
    "    \"num_epochs\": 50,\n",
    "    \"batch_size\": 16,\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"warmup_steps\": 150,\n",
    "    \"max_input_length\": 128,\n",
    "    \"max_output_length\": 32,\n",
    "    \"train_ratio\": 0.8,\n",
    "    \"val_ratio\": 0.1,\n",
    "    \"test_ratio\": 0.1,\n",
    "    \"fp16\": False,  # NaNé˜²æ­¢ã®ãŸã‚ã‚ªãƒ•\n",
    "    \"logging_steps\": 50,\n",
    "    \"label_smoothing\": 0.1,  # mode collapseå¯¾ç­–\n",
    "}\n",
    "\n",
    "print(\"Config:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af25670",
   "metadata": {},
   "source": [
    "## 3. ãƒ‡ãƒ¼ã‚¿æº–å‚™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c847585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/content/Jmoji\")\n",
    "\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "def load_jsonl(path):\n",
    "    data = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "def split_dataset(samples, train_ratio, val_ratio, seed=42):\n",
    "    data = list(samples)\n",
    "    random.seed(seed)\n",
    "    random.shuffle(data)\n",
    "    n = len(data)\n",
    "    train_end = int(n * train_ratio)\n",
    "    val_end = int(n * (train_ratio + val_ratio))\n",
    "    return data[:train_end], data[train_end:val_end], data[val_end:]\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿\n",
    "samples = load_jsonl(DATA_PATH)\n",
    "print(f\"Total samples: {len(samples)}\")\n",
    "\n",
    "# åˆ†å‰²\n",
    "train_samples, val_samples, test_samples = split_dataset(\n",
    "    samples,\n",
    "    CONFIG[\"train_ratio\"],\n",
    "    CONFIG[\"val_ratio\"]\n",
    ")\n",
    "print(f\"Train: {len(train_samples)}, Val: {len(val_samples)}, Test: {len(test_samples)}\")\n",
    "\n",
    "# ã‚µãƒ³ãƒ—ãƒ«ç¢ºèª\n",
    "print(\"\\nSample data:\")\n",
    "for i, s in enumerate(train_samples[:3]):\n",
    "    print(f\"  [{i}] {s['sns_text'][:50]}... -> {s['emoji_string']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5cf0db",
   "metadata": {},
   "source": [
    "## 3.5 çµµæ–‡å­—åˆ†å¸ƒã®ç¢ºèª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efddd20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# å…¨ã‚µãƒ³ãƒ—ãƒ«ã®çµµæ–‡å­—ã‚’é›†è¨ˆ\n",
    "all_emojis = []\n",
    "for sample in samples:\n",
    "    emojis = sample[\"emoji_string\"].split()\n",
    "    all_emojis.extend(emojis)\n",
    "\n",
    "# é »åº¦ã‚«ã‚¦ãƒ³ãƒˆ\n",
    "emoji_counts = Counter(all_emojis)\n",
    "print(f\"Total emoji occurrences: {len(all_emojis)}\")\n",
    "print(f\"Unique emojis: {len(emoji_counts)}\")\n",
    "print(\"\\nTop 20 emojis:\")\n",
    "for emoji, count in emoji_counts.most_common(20):\n",
    "    pct = count / len(all_emojis) * 100\n",
    "    print(f\"  {emoji}: {count} ({pct:.1f}%)\")\n",
    "\n",
    "# æœ€é »å‡ºçµµæ–‡å­—ã®å‰²åˆã‚’è­¦å‘Š\n",
    "top_emoji, top_count = emoji_counts.most_common(1)[0]\n",
    "top_pct = top_count / len(all_emojis) * 100\n",
    "if top_pct > 15:\n",
    "    print(f\"\\nâš ï¸ Warning: '{top_emoji}' is {top_pct:.1f}% of all emojis. This may cause mode collapse.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbd0168",
   "metadata": {},
   "source": [
    "## 4. ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶æº–å‚™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea2796f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from src.data.emoji_utils import get_all_emojis\n",
    "\n",
    "def setup_model_with_emoji_tokens(model_name):\n",
    "    \"\"\"çµµæ–‡å­—ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’æº–å‚™\"\"\"\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name, legacy=False)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    # çµµæ–‡å­—ã‚’ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã¨ã—ã¦è¿½åŠ \n",
    "    emoji_tokens = list(get_all_emojis())\n",
    "    num_added = tokenizer.add_tokens(emoji_tokens)\n",
    "    print(f\"Added {num_added} emoji tokens\")\n",
    "\n",
    "    # åŸ‹ã‚è¾¼ã¿å±¤ã‚’ãƒªã‚µã‚¤ã‚º\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    print(f\"Vocab size: {len(tokenizer)}\")\n",
    "\n",
    "    return tokenizer, model\n",
    "\n",
    "tokenizer, model = setup_model_with_emoji_tokens(CONFIG[\"model_name\"])\n",
    "\n",
    "# çµµæ–‡å­—ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ç¢ºèª\n",
    "test_emoji = \"ğŸ˜Š ğŸ‰\"\n",
    "ids = tokenizer.encode(test_emoji, add_special_tokens=False)\n",
    "decoded = tokenizer.decode(ids)\n",
    "print(f\"Emoji tokenization test: '{test_emoji}' -> {ids} -> '{decoded}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1756407f",
   "metadata": {},
   "source": [
    "## 5. Datasetæº–å‚™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5ab284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class EmojiDataset(Dataset):\n",
    "    def __init__(self, samples, tokenizer, max_input_length=128, max_output_length=32):\n",
    "        self.samples = list(samples)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_output_length = max_output_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        input_text = sample[\"sns_text\"]\n",
    "        output_text = sample[\"emoji_string\"]\n",
    "\n",
    "        input_encoding = self.tokenizer(\n",
    "            input_text,\n",
    "            max_length=self.max_input_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        output_encoding = self.tokenizer(\n",
    "            output_text,\n",
    "            max_length=self.max_output_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã‚’-100ã«ç½®ãæ›ãˆï¼ˆæå¤±è¨ˆç®—ã‹ã‚‰é™¤å¤–ï¼‰\n",
    "        labels = output_encoding[\"input_ids\"].squeeze(0).clone()\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": input_encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "\n",
    "# Datasetä½œæˆ\n",
    "train_dataset = EmojiDataset(\n",
    "    train_samples, tokenizer,\n",
    "    CONFIG[\"max_input_length\"],\n",
    "    CONFIG[\"max_output_length\"]\n",
    ")\n",
    "val_dataset = EmojiDataset(\n",
    "    val_samples, tokenizer,\n",
    "    CONFIG[\"max_input_length\"],\n",
    "    CONFIG[\"max_output_length\"]\n",
    ")\n",
    "test_dataset = EmojiDataset(\n",
    "    test_samples, tokenizer,\n",
    "    CONFIG[\"max_input_length\"],\n",
    "    CONFIG[\"max_output_length\"]\n",
    ")\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)}\")\n",
    "print(f\"Val dataset: {len(val_dataset)}\")\n",
    "print(f\"Test dataset: {len(test_dataset)}\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ç¢ºèª\n",
    "item = train_dataset[0]\n",
    "print(f\"\\nSample item shapes:\")\n",
    "print(f\"  input_ids: {item['input_ids'].shape}\")\n",
    "print(f\"  attention_mask: {item['attention_mask'].shape}\")\n",
    "print(f\"  labels: {item['labels'].shape}\")\n",
    "print(f\"  Non -100 labels: {(item['labels'] != -100).sum().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0987a00",
   "metadata": {},
   "source": [
    "## 6. å­¦ç¿’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b236ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import os\n",
    "\n",
    "# å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(EVAL_DIR, exist_ok=True)\n",
    "\n",
    "# TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=CONFIG[\"num_epochs\"],\n",
    "    per_device_train_batch_size=CONFIG[\"batch_size\"],\n",
    "    per_device_eval_batch_size=CONFIG[\"batch_size\"],\n",
    "    learning_rate=CONFIG[\"learning_rate\"],\n",
    "    weight_decay=CONFIG[\"weight_decay\"],\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    logging_steps=CONFIG[\"logging_steps\"],\n",
    "    warmup_steps=CONFIG[\"warmup_steps\"],\n",
    "    fp16=CONFIG[\"fp16\"],\n",
    "    label_smoothing_factor=CONFIG[\"label_smoothing\"],  # mode collapseå¯¾ç­–\n",
    "    report_to=\"none\",  # wandbã‚’ç„¡åŠ¹åŒ–\n",
    "    save_total_limit=3,  # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ•°ã‚’åˆ¶é™\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# GPUç§»å‹•\n",
    "if torch.cuda.is_available():\n",
    "    model.to(\"cuda\")\n",
    "\n",
    "print(\"Starting training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1067e322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ç¿’å®Ÿè¡Œ\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a34c721",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ãƒ¢ãƒ‡ãƒ«ä¿å­˜\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"Model saved to {OUTPUT_DIR}\")\n",
    "\n",
    "# è©•ä¾¡çµæœä¿å­˜\n",
    "eval_result = trainer.evaluate()\n",
    "print(f\"\\nEval results: {eval_result}\")\n",
    "\n",
    "with open(f\"{EVAL_DIR}/train_eval_results.txt\", \"w\") as f:\n",
    "    f.write(str(eval_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d63351",
   "metadata": {},
   "source": [
    "## 7. æ¨è«–ãƒ†ã‚¹ãƒˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8af275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_emoji(model, tokenizer, text, max_length=32, use_sampling=True):\n",
    "    \"\"\"ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰çµµæ–‡å­—ã‚’ç”Ÿæˆ\"\"\"\n",
    "    model.eval()\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=128,\n",
    "        truncation=True\n",
    "    )\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if use_sampling:\n",
    "            # Temperature samplingï¼ˆå¤šæ§˜æ€§é‡è¦–ï¼‰\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_length=max_length,\n",
    "                do_sample=True,\n",
    "                temperature=1.0,\n",
    "                top_k=50,\n",
    "                top_p=0.95,\n",
    "            )\n",
    "        else:\n",
    "            # Beam searchï¼ˆç²¾åº¦é‡è¦–ï¼‰\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_length=max_length,\n",
    "                num_beams=4,\n",
    "                early_stopping=True\n",
    "            )\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã§ãƒ†ã‚¹ãƒˆï¼ˆæš—è¨˜ç¢ºèªï¼‰\n",
    "print(\"=== å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã§ã®ãƒ†ã‚¹ãƒˆï¼ˆSamplingï¼‰ ===\")\n",
    "for sample in train_samples[:5]:\n",
    "    text = sample[\"sns_text\"]\n",
    "    expected = sample[\"emoji_string\"]\n",
    "    result = generate_emoji(model, tokenizer, text, use_sampling=True)\n",
    "    match = \"OK\" if result.strip() == expected.strip() else \"NG\"\n",
    "    print(f\"[{match}] å…¥åŠ›: {text[:40]}...\")\n",
    "    print(f\"     æœŸå¾…: {expected}\")\n",
    "    print(f\"     å‡ºåŠ›: {result}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7854b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beam search ã¨ã®æ¯”è¼ƒ\n",
    "print(\"=== Beam Search vs Sampling æ¯”è¼ƒ ===\")\n",
    "for sample in train_samples[:3]:\n",
    "    text = sample[\"sns_text\"]\n",
    "    expected = sample[\"emoji_string\"]\n",
    "    result_beam = generate_emoji(model, tokenizer, text, use_sampling=False)\n",
    "    result_sample = generate_emoji(model, tokenizer, text, use_sampling=True)\n",
    "    print(f\"å…¥åŠ›: {text[:40]}...\")\n",
    "    print(f\"  æœŸå¾…: {expected}\")\n",
    "    print(f\"  Beam: {result_beam}\")\n",
    "    print(f\"  Sample: {result_sample}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0bcac9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# æ–°è¦ãƒ†ã‚­ã‚¹ãƒˆã§ãƒ†ã‚¹ãƒˆ\n",
    "print(\"=== æ–°è¦ãƒ†ã‚­ã‚¹ãƒˆã§ã®ãƒ†ã‚¹ãƒˆ ===\")\n",
    "test_texts = [\n",
    "    \"ä»Šæ—¥ã¯æ¥½ã—ã‹ã£ãŸ\",\n",
    "    \"æ˜æ—¥ã¯é›¨ã‚‰ã—ã„\",\n",
    "    \"ãŠãªã‹ã™ã„ãŸ\",\n",
    "    \"è©¦é¨“ã«åˆæ ¼ã—ãŸ\",\n",
    "    \"æ¨ã—ãŒå°Šã„\",\n",
    "    \"ã‚ã£ã¡ã‚ƒçœ ã„\",\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    result = generate_emoji(model, tokenizer, text, use_sampling=True)\n",
    "    print(f\"å…¥åŠ›: {text}\")\n",
    "    print(f\"å‡ºåŠ›: {result}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695684e7",
   "metadata": {},
   "source": [
    "## 8. è©•ä¾¡æŒ‡æ¨™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f26764d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(pred_set, gold_set):\n",
    "    \"\"\"Jaccardé¡ä¼¼åº¦ã‚’è¨ˆç®—\"\"\"\n",
    "    if not pred_set and not gold_set:\n",
    "        return 1.0\n",
    "    if not pred_set or not gold_set:\n",
    "        return 0.0\n",
    "    intersection = len(pred_set & gold_set)\n",
    "    union = len(pred_set | gold_set)\n",
    "    return intersection / union\n",
    "\n",
    "def evaluate_model(model, tokenizer, samples, max_samples=100):\n",
    "    \"\"\"ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡\"\"\"\n",
    "    results = []\n",
    "\n",
    "    for sample in samples[:max_samples]:\n",
    "        text = sample[\"sns_text\"]\n",
    "        gold = sample[\"emoji_string\"]\n",
    "        pred = generate_emoji(model, tokenizer, text)\n",
    "\n",
    "        # çµµæ–‡å­—ã‚’ã‚»ãƒƒãƒˆã«å¤‰æ›\n",
    "        gold_set = set(gold.split())\n",
    "        pred_set = set(pred.split())\n",
    "\n",
    "        jaccard = jaccard_similarity(pred_set, gold_set)\n",
    "        exact_match = 1 if gold_set == pred_set else 0\n",
    "\n",
    "        results.append({\n",
    "            \"jaccard\": jaccard,\n",
    "            \"exact_match\": exact_match,\n",
    "            \"pred\": pred,\n",
    "            \"gold\": gold,\n",
    "        })\n",
    "\n",
    "    # é›†è¨ˆ\n",
    "    avg_jaccard = sum(r[\"jaccard\"] for r in results) / len(results)\n",
    "    exact_match_rate = sum(r[\"exact_match\"] for r in results) / len(results)\n",
    "\n",
    "    return {\n",
    "        \"avg_jaccard\": avg_jaccard,\n",
    "        \"exact_match_rate\": exact_match_rate,\n",
    "        \"num_samples\": len(results),\n",
    "        \"details\": results,\n",
    "    }\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§è©•ä¾¡\n",
    "print(\"=== ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆè©•ä¾¡ ===\")\n",
    "eval_results = evaluate_model(model, tokenizer, test_samples)\n",
    "print(f\"Average Jaccard: {eval_results['avg_jaccard']:.4f}\")\n",
    "print(f\"Exact Match Rate: {eval_results['exact_match_rate']:.4f}\")\n",
    "print(f\"Samples evaluated: {eval_results['num_samples']}\")\n",
    "\n",
    "# çµæœä¿å­˜\n",
    "import json\n",
    "with open(f\"{EVAL_DIR}/test_metrics.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"avg_jaccard\": eval_results[\"avg_jaccard\"],\n",
    "        \"exact_match_rate\": eval_results[\"exact_match_rate\"],\n",
    "        \"num_samples\": eval_results[\"num_samples\"],\n",
    "    }, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acb42ed",
   "metadata": {},
   "source": [
    "## 9. ãƒ¢ãƒ‡ãƒ«ã‚’Driveã«ä¿å­˜ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4ffeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Driveã«ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜\n",
    "DRIVE_MODEL_PATH = \"/content/drive/MyDrive/school/ai_application/jmoji_model\"\n",
    "\n",
    "import shutil\n",
    "shutil.copytree(OUTPUT_DIR, DRIVE_MODEL_PATH, dirs_exist_ok=True)\n",
    "print(f\"Model copied to {DRIVE_MODEL_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
