{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ddb89d7",
   "metadata": {},
   "source": [
    "# Jmoji T5 Training on Google Colab\n",
    "\n",
    "æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆâ†’çµµæ–‡å­—ç¿»è¨³ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64cc352",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3454463d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Driveã‚’ãƒã‚¦ãƒ³ãƒˆ\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89ee0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒªãƒã‚¸ãƒˆãƒªã‚¯ãƒ­ãƒ¼ãƒ³\n",
    "!git clone https://github.com/AtefAndrus/Jmoji.git\n",
    "%cd /content/Jmoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a680aca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆpyproject.tomlã‹ã‚‰è‡ªå‹•è§£æ±ºï¼‰\n",
    "!pip install -q ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4c1a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPUç¢ºèª\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7d6da9",
   "metadata": {},
   "source": [
    "## 2. è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aeb57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‘ã‚¹è¨­å®š\n",
    "DATA_PATH = \"/content/drive/MyDrive/school/ai_application/dataset_v3.jsonl\"\n",
    "OUTPUT_DIR = \"/content/Jmoji/outputs/models\"\n",
    "EVAL_DIR = \"/content/Jmoji/outputs/evaluation\"\n",
    "\n",
    "# å­¦ç¿’è¨­å®š\n",
    "CONFIG = {\n",
    "    \"model_name\": \"sonoisa/t5-base-japanese\",\n",
    "    \"num_epochs\": 50,\n",
    "    \"batch_size\": 16,\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"warmup_steps\": 150,\n",
    "    \"max_input_length\": 128,\n",
    "    \"max_output_length\": 32,\n",
    "    \"train_ratio\": 0.8,\n",
    "    \"val_ratio\": 0.1,\n",
    "    \"test_ratio\": 0.1,\n",
    "    \"fp16\": False,  # NaNé˜²æ­¢ã®ãŸã‚ã‚ªãƒ•\n",
    "    \"logging_steps\": 50,\n",
    "    \"label_smoothing\": 0.1,  # mode collapseå¯¾ç­–\n",
    "    \"early_stopping_patience\": 5,\n",
    "}\n",
    "\n",
    "print(\"Config:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650e4ba6",
   "metadata": {},
   "source": [
    "## 3. ãƒ‡ãƒ¼ã‚¿æº–å‚™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb0be78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/content/Jmoji\")\n",
    "\n",
    "from src.models.t5_trainer import (\n",
    "    EmojiDataset,\n",
    "    TrainConfig,\n",
    "    setup_model_with_emoji_tokens,\n",
    "    build_trainer,\n",
    "    split_dataset,\n",
    "    load_jsonl,\n",
    "    generate_emoji,\n",
    "    evaluate_model,\n",
    ")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿\n",
    "samples = load_jsonl(DATA_PATH)\n",
    "print(f\"Total samples: {len(samples)}\")\n",
    "\n",
    "# åˆ†å‰²\n",
    "train_samples, val_samples, test_samples = split_dataset(\n",
    "    samples,\n",
    "    CONFIG[\"train_ratio\"],\n",
    "    CONFIG[\"val_ratio\"]\n",
    ")\n",
    "print(f\"Train: {len(train_samples)}, Val: {len(val_samples)}, Test: {len(test_samples)}\")\n",
    "\n",
    "# ã‚µãƒ³ãƒ—ãƒ«ç¢ºèª\n",
    "print(\"\\nSample data:\")\n",
    "for i, s in enumerate(train_samples[:3]):\n",
    "    print(f\"  [{i}] {s['sns_text'][:50]}... -> {s['emoji_string']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd0cbfd",
   "metadata": {},
   "source": [
    "## 3.5 çµµæ–‡å­—åˆ†å¸ƒã®ç¢ºèª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c003470c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# å…¨ã‚µãƒ³ãƒ—ãƒ«ã®çµµæ–‡å­—ã‚’é›†è¨ˆ\n",
    "all_emojis = []\n",
    "for sample in samples:\n",
    "    emojis = sample[\"emoji_string\"].split()\n",
    "    all_emojis.extend(emojis)\n",
    "\n",
    "# é »åº¦ã‚«ã‚¦ãƒ³ãƒˆ\n",
    "emoji_counts = Counter(all_emojis)\n",
    "print(f\"Total emoji occurrences: {len(all_emojis)}\")\n",
    "print(f\"Unique emojis: {len(emoji_counts)}\")\n",
    "print(\"\\nTop 20 emojis:\")\n",
    "for emoji, count in emoji_counts.most_common(20):\n",
    "    pct = count / len(all_emojis) * 100\n",
    "    print(f\"  {emoji}: {count} ({pct:.1f}%)\")\n",
    "\n",
    "# æœ€é »å‡ºçµµæ–‡å­—ã®å‰²åˆã‚’è­¦å‘Š\n",
    "top_emoji, top_count = emoji_counts.most_common(1)[0]\n",
    "top_pct = top_count / len(all_emojis) * 100\n",
    "if top_pct > 15:\n",
    "    print(f\"\\nWarning: '{top_emoji}' is {top_pct:.1f}% of all emojis. This may cause mode collapse.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf745b28",
   "metadata": {},
   "source": [
    "## 4. ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶æº–å‚™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801cdfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, model = setup_model_with_emoji_tokens(CONFIG[\"model_name\"])\n",
    "print(f\"Vocab size: {len(tokenizer)}\")\n",
    "\n",
    "# çµµæ–‡å­—ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ç¢ºèª\n",
    "test_emoji = \"ğŸ˜Š ğŸ‰\"\n",
    "ids = tokenizer.encode(test_emoji, add_special_tokens=False)\n",
    "decoded = tokenizer.decode(ids)\n",
    "print(f\"Emoji tokenization test: '{test_emoji}' -> {ids} -> '{decoded}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d207af",
   "metadata": {},
   "source": [
    "## 5. Datasetæº–å‚™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667b8125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasetä½œæˆ\n",
    "train_dataset = EmojiDataset(\n",
    "    train_samples, tokenizer,\n",
    "    CONFIG[\"max_input_length\"],\n",
    "    CONFIG[\"max_output_length\"]\n",
    ")\n",
    "val_dataset = EmojiDataset(\n",
    "    val_samples, tokenizer,\n",
    "    CONFIG[\"max_input_length\"],\n",
    "    CONFIG[\"max_output_length\"]\n",
    ")\n",
    "test_dataset = EmojiDataset(\n",
    "    test_samples, tokenizer,\n",
    "    CONFIG[\"max_input_length\"],\n",
    "    CONFIG[\"max_output_length\"]\n",
    ")\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)}\")\n",
    "print(f\"Val dataset: {len(val_dataset)}\")\n",
    "print(f\"Test dataset: {len(test_dataset)}\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ç¢ºèª\n",
    "item = train_dataset[0]\n",
    "print(f\"\\nSample item shapes:\")\n",
    "print(f\"  input_ids: {item['input_ids'].shape}\")\n",
    "print(f\"  attention_mask: {item['attention_mask'].shape}\")\n",
    "print(f\"  labels: {item['labels'].shape}\")\n",
    "print(f\"  Non -100 labels: {(item['labels'] != -100).sum().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2467f5",
   "metadata": {},
   "source": [
    "## 6. å­¦ç¿’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fe27de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(EVAL_DIR, exist_ok=True)\n",
    "\n",
    "# TrainConfigã‚’æ§‹ç¯‰\n",
    "train_config = TrainConfig(\n",
    "    model_name=CONFIG[\"model_name\"],\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=CONFIG[\"num_epochs\"],\n",
    "    per_device_train_batch_size=CONFIG[\"batch_size\"],\n",
    "    per_device_eval_batch_size=CONFIG[\"batch_size\"],\n",
    "    learning_rate=CONFIG[\"learning_rate\"],\n",
    "    weight_decay=CONFIG[\"weight_decay\"],\n",
    "    warmup_steps=CONFIG[\"warmup_steps\"],\n",
    "    logging_steps=CONFIG[\"logging_steps\"],\n",
    "    fp16=CONFIG[\"fp16\"],\n",
    "    label_smoothing_factor=CONFIG[\"label_smoothing\"],\n",
    "    early_stopping_patience=CONFIG[\"early_stopping_patience\"],\n",
    "    save_total_limit=3,\n",
    ")\n",
    "\n",
    "# Traineræ§‹ç¯‰\n",
    "trainer = build_trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    cfg=train_config,\n",
    ")\n",
    "\n",
    "# GPUç§»å‹•\n",
    "if torch.cuda.is_available():\n",
    "    model.to(\"cuda\")\n",
    "\n",
    "print(\"Starting training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc780e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ç¿’å®Ÿè¡Œ\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c942bce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ¢ãƒ‡ãƒ«ä¿å­˜\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"Model saved to {OUTPUT_DIR}\")\n",
    "\n",
    "# è©•ä¾¡çµæœä¿å­˜\n",
    "eval_result = trainer.evaluate()\n",
    "print(f\"\\nEval results: {eval_result}\")\n",
    "\n",
    "with open(f\"{EVAL_DIR}/train_eval_results.txt\", \"w\") as f:\n",
    "    f.write(str(eval_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c9fdf3",
   "metadata": {},
   "source": [
    "## 7. æ¨è«–ãƒ†ã‚¹ãƒˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2791771f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã§ãƒ†ã‚¹ãƒˆï¼ˆæš—è¨˜ç¢ºèªï¼‰\n",
    "print(\"=== å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã§ã®ãƒ†ã‚¹ãƒˆï¼ˆSamplingï¼‰ ===\")\n",
    "for sample in train_samples[:5]:\n",
    "    text = sample[\"sns_text\"]\n",
    "    expected = sample[\"emoji_string\"]\n",
    "    result = generate_emoji(model, tokenizer, text, use_sampling=True)\n",
    "    match = \"OK\" if result.strip() == expected.strip() else \"NG\"\n",
    "    print(f\"[{match}] å…¥åŠ›: {text[:40]}...\")\n",
    "    print(f\"     æœŸå¾…: {expected}\")\n",
    "    print(f\"     å‡ºåŠ›: {result}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719f22df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beam search ã¨ã®æ¯”è¼ƒ\n",
    "print(\"=== Beam Search vs Sampling æ¯”è¼ƒ ===\")\n",
    "for sample in train_samples[:3]:\n",
    "    text = sample[\"sns_text\"]\n",
    "    expected = sample[\"emoji_string\"]\n",
    "    result_beam = generate_emoji(model, tokenizer, text, use_sampling=False)\n",
    "    result_sample = generate_emoji(model, tokenizer, text, use_sampling=True)\n",
    "    print(f\"å…¥åŠ›: {text[:40]}...\")\n",
    "    print(f\"  æœŸå¾…: {expected}\")\n",
    "    print(f\"  Beam: {result_beam}\")\n",
    "    print(f\"  Sample: {result_sample}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ffb272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ–°è¦ãƒ†ã‚­ã‚¹ãƒˆã§ãƒ†ã‚¹ãƒˆ\n",
    "print(\"=== æ–°è¦ãƒ†ã‚­ã‚¹ãƒˆã§ã®ãƒ†ã‚¹ãƒˆ ===\")\n",
    "test_texts = [\n",
    "    \"ä»Šæ—¥ã¯æ¥½ã—ã‹ã£ãŸ\",\n",
    "    \"æ˜æ—¥ã¯é›¨ã‚‰ã—ã„\",\n",
    "    \"ãŠãªã‹ã™ã„ãŸ\",\n",
    "    \"è©¦é¨“ã«åˆæ ¼ã—ãŸ\",\n",
    "    \"æ¨ã—ãŒå°Šã„\",\n",
    "    \"ã‚ã£ã¡ã‚ƒçœ ã„\",\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    result = generate_emoji(model, tokenizer, text, use_sampling=True)\n",
    "    print(f\"å…¥åŠ›: {text}\")\n",
    "    print(f\"å‡ºåŠ›: {result}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa6da9c",
   "metadata": {},
   "source": [
    "## 8. è©•ä¾¡æŒ‡æ¨™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56372c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§è©•ä¾¡ï¼ˆå…¨ä»¶ï¼‰\n",
    "print(\"=== ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆè©•ä¾¡ ===\")\n",
    "eval_results = evaluate_model(model, tokenizer, test_samples)\n",
    "print(f\"Average Jaccard: {eval_results.avg_jaccard:.4f}\")\n",
    "print(f\"Exact Match Rate: {eval_results.exact_match_rate:.4f}\")\n",
    "print(f\"Micro F1: {eval_results.micro_f1:.4f}\")\n",
    "print(f\"Avg Precision: {eval_results.avg_precision:.4f}\")\n",
    "print(f\"Avg Recall: {eval_results.avg_recall:.4f}\")\n",
    "print(f\"Avg F1: {eval_results.avg_f1:.4f}\")\n",
    "print(f\"Samples evaluated: {eval_results.num_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ef827f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# çµæœä¿å­˜\n",
    "import json\n",
    "with open(f\"{EVAL_DIR}/test_metrics.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"avg_jaccard\": eval_results.avg_jaccard,\n",
    "        \"exact_match_rate\": eval_results.exact_match_rate,\n",
    "        \"micro_f1\": eval_results.micro_f1,\n",
    "        \"avg_precision\": eval_results.avg_precision,\n",
    "        \"avg_recall\": eval_results.avg_recall,\n",
    "        \"avg_f1\": eval_results.avg_f1,\n",
    "        \"num_samples\": eval_results.num_samples,\n",
    "    }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Metrics saved to {EVAL_DIR}/test_metrics.json\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
