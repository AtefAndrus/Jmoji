#!/usr/bin/env python3
"""Repetition penalty„ÅÆÂäπÊûú„Çí„ÉÜ„Çπ„Éà„Åô„Çã„Çπ„ÇØ„É™„Éó„Éà„ÄÇ"""

import torch
from transformers import T5ForConditionalGeneration, T5Tokenizer

# „ÉÜ„Çπ„ÉàÁî®„Çµ„É≥„Éó„É´ÔºàÈÅéÂâ∞ÁîüÊàê„ÅåË¶ã„Çâ„Çå„Åü„ÇÇ„ÅÆÔºâ
TEST_SAMPLES = [
    {
        "text": "Ëá™ÊÄßÂàÜÂà•„Å£„Å¶„ÅÆ„ÅØ„ÄÅÂ∞ã„Å®‰º∫„ÅÆ„Åì„Å®„Åß„ÄÅÂàÜÂà•„ÅÆ‰∏ÄÁ®Æ„Å£„Å¶„Åì„Å®„Å≠„ÄÇ",
        "gold": "ü§î üß† üìñ ‚ú® üîç",
        "original_pred": "üòä üòä üòä üòä üòä",  # v4_focal_top50
    },
    {
        "text": "Ê∏ØÂå∫„ÅÆÂ§ß‰ºö„Åß‰∏ä‰ΩçÂÖ•Ë≥û„Åó„Åü„Çä„Åó„Å¶„ÄÅÁµêÊßã„Ç¢„Éî„Éº„É´„Åó„Åü„Çè„Äú",
        "gold": "üèÜ üéâ üëè üáØüáµ üí™",
        "original_pred": "üì∫ üòä üòä üòä üòä",  # v4_focal_top50
    },
    {
        "text": "‰∏âÊú®ÊîπÈÄ†ÂÜÖÈñ£„ÅÆÊôÇ„Å´„ÄÅËá™Ê∞ëÂÖö„ÅÆÂÖö‰∏âÂΩπÔºàÂππ‰∫ãÈï∑„ÄÅÊîøË™ø‰ºöÈï∑„ÄÅÁ∑èÂãô‰ºöÈï∑Ôºâ„Å£„Å¶„ÄÅ‰∏ªÊµÅÊ¥æ„Åò„ÇÉ„Å™„ÅÑ„Äå‰∏âÊú®„Åä„Çç„Åó„Äç„ÅÆ‰∏≠ÂøÉ„Å†„Å£„ÅüÊåôÂÖöÂçî„Å´„ÅØÂ±û„Åó„Å¶„Å™„ÅÑ‰∫∫„ÇíÈñ£ÂÉö„Å´ÊäúÊì¢„Åó„Åü„Çì„Å†„Å£„Å¶„ÄÇ",
        "gold": "ü§î üáØüáµ üèõÔ∏è üí¨ ü§ù",
        "original_pred": "üëè üëè üìñ üëè üëè",  # v4_focal_top50
    },
    {
        "text": "Â§ßÁ¥´„Å£„Å¶„ÅÆ„ÅØÂ£¨Áî≥„ÅÆÂäüËá£„ÅÆ‰∏≠„Åß„ÇÇ„Åë„Å£„Åì„ÅÜ‰∏ä„ÅÆ„ÇØ„É©„Çπ„Å™„Çì„Å†„Åë„Å©„ÄÅ„ÄéÊõ∏Á¥Ä„Äè„ÅÆÂ£¨Áî≥„ÅÆ‰π±„ÅÆ„Å®„ÅìË¶ã„Å¶„ÇÇÊòüÂ∑ùÈ∫ªÂëÇ„ÅÆÂêçÂâçÂá∫„Å¶„Åì„Å™„ÅÑ„Åã„Çâ„ÄÅÁµêÂ±Ä„Å©„Çì„Å™Ê¥ªË∫ç„Åó„Åü„ÅÆ„Åã„ÅØ„Çà„Åè„Çè„Åã„Çì„Å™„ÅÑ„Çì„Å†„Çà„Å™„Äú„ÄÇ",
        "gold": "ü§î üìö üìñ üáØüáµ üîç",
        "original_pred": "üìñ üòä üáØüáµ üìö üëè",  # v4_focal_top50
    },
    {
        "text": "Âàù„ÅÆ„Ç™„Éº„É´„Çπ„Çø„Éº„Ç≤„Éº„É†Âá∫Â†¥„Åß„ÄÅ7Êúà11Êó•„ÅÆÁ¨¨1Êà¶Ôºà„Ç≠„É£„É≥„Éâ„É´„Çπ„ÉÜ„Ç£„ÉÉ„ÇØÔºâ„ÅÆ8Âõû„Å´‰ª£Êâì„ÅßÁôªÂ†¥„ÄÇ„Éû„Ç§„ÇØ„Éª„Éï„Ç©„Éº„Éã„É¨„Çπ„Åã„Çâ„ÅÑ„Åç„Å™„ÇäÂàùÊâìÂ∏≠Êú¨Â°ÅÊâìÔºÅ",
        "gold": "‚öæ üéâ üëè üî• üí•",
        "original_pred": "üìö üìö üòä üìñ üìö",  # v4_focal_top50
    },
]


def generate_with_penalty(
    model: T5ForConditionalGeneration,
    tokenizer: T5Tokenizer,
    text: str,
    repetition_penalty: float = 1.0,
    device: str = "cpu",
) -> str:
    """repetition_penalty„ÇíÈÅ©Áî®„Åó„Å¶ÁîüÊàê„ÄÇ"""
    model.eval()
    inputs = tokenizer(text, return_tensors="pt", max_length=128, truncation=True)
    inputs = {k: v.to(device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=32,
            do_sample=True,
            temperature=1.0,
            top_k=50,
            top_p=0.95,
            repetition_penalty=repetition_penalty,
        )

    return tokenizer.decode(outputs[0], skip_special_tokens=True)


def main():
    import sys

    print("=" * 60)
    print("Repetition Penalty „ÉÜ„Çπ„Éà")
    print("=" * 60)

    # „Ç≥„Éû„É≥„Éâ„É©„Ç§„É≥ÂºïÊï∞„Åß„É¢„Éá„É´„ÇíÂàá„ÇäÊõø„Åà
    if len(sys.argv) > 1 and sys.argv[1] == "top50":
        model_name = "AtefAndrus/jmoji-t5-v4_top50_20251224"
    else:
        model_name = "AtefAndrus/jmoji-t5-v4_focal_top50_20251224"

    print(f"\n„É¢„Éá„É´„Çí„É≠„Éº„Éâ‰∏≠: {model_name}")

    tokenizer = T5Tokenizer.from_pretrained(model_name, legacy=False)
    model = T5ForConditionalGeneration.from_pretrained(model_name)

    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = model.to(device)
    print(f"„Éá„Éê„Ç§„Çπ: {device}")

    # „ÉÜ„Çπ„Éà„Åô„Çãrepetition_penaltyÂÄ§
    penalties = [1.0, 1.2, 1.5, 2.0]

    print("\n" + "=" * 60)

    for i, sample in enumerate(TEST_SAMPLES, 1):
        print(f"\n### Sample {i}")
        print(f"Text: {sample['text'][:50]}...")
        print(f"Gold: {sample['gold']}")
        print(f"Original (penalty=1.0): {sample['original_pred']}")
        print()

        for penalty in penalties:
            pred = generate_with_penalty(
                model,
                tokenizer,
                sample["text"],
                repetition_penalty=penalty,
                device=device,
            )
            # ÈáçË§á„Çí„Ç´„Ç¶„É≥„Éà
            emojis = pred.split()
            unique_count = len(set(emojis))
            total_count = len(emojis)
            print(f"  penalty={penalty}: {pred} (unique: {unique_count}/{total_count})")

        print("-" * 40)

    print("\n" + "=" * 60)
    print("„ÉÜ„Çπ„ÉàÂÆå‰∫Ü")


if __name__ == "__main__":
    main()
