from pathlib import Path
from unittest.mock import MagicMock, patch

import pytest
import torch

from src.models.t5_trainer import (
    EmojiDataset,
    EvaluationResult,
    TrainConfig,
    build_trainer,
    evaluate_model,
    generate_emoji,
    load_jsonl,
    split_dataset,
)


class FakeTokenizer:
    pad_token_id = 0

    def __call__(
        self,
        text,
        max_length=10,
        padding="max_length",
        truncation=True,
        return_tensors="pt",
    ):
        length = min(len(text), max_length)
        ids = list(range(length)) + [0] * (max_length - length)
        attn = [1] * length + [0] * (max_length - length)
        return {
            "input_ids": torch.tensor([ids]),
            "attention_mask": torch.tensor([attn]),
        }


def test_emoji_dataset_shapes():
    samples = [
        {"sns_text": "ä»Šæ—¥ã¯æ¥½ã—ã„", "emoji_string": "ğŸ˜Š ğŸ‰"},
        {"sns_text": "æ˜æ—¥ã¯æ™´ã‚Œ", "emoji_string": "â˜€ï¸"},
    ]
    tok = FakeTokenizer()
    ds = EmojiDataset(samples, tok, max_input_length=8, max_output_length=4)
    item = ds[0]
    assert item["input_ids"].shape == (8,)
    assert item["attention_mask"].shape == (8,)
    assert item["labels"].shape == (4,)


def test_split_dataset():
    samples = [{"id": i} for i in range(10)]
    train, val, test = split_dataset(samples, 0.6, 0.2)
    assert len(train) == 6
    assert len(val) == 2
    assert len(test) == 2


def test_split_dataset_shuffle_with_seed():
    """åŒã˜seedã§åŒã˜çµæœã€ç•°ãªã‚‹seedã§ç•°ãªã‚‹çµæœã«ãªã‚‹ã“ã¨ã‚’ç¢ºèª"""
    samples = [{"id": i} for i in range(100)]

    train1, _, _ = split_dataset(samples, 0.8, 0.1, seed=42)
    train2, _, _ = split_dataset(samples, 0.8, 0.1, seed=42)
    train3, _, _ = split_dataset(samples, 0.8, 0.1, seed=123)

    assert train1 == train2  # åŒã˜seedãªã‚‰åŒã˜çµæœ
    assert train1 != train3  # ç•°ãªã‚‹seedãªã‚‰ç•°ãªã‚‹çµæœ


def test_split_dataset_no_shuffle():
    """shuffle=Falseã§é †åºãŒä¿æŒã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª"""
    samples = [{"id": i} for i in range(10)]
    train, val, test = split_dataset(samples, 0.6, 0.2, shuffle=False)

    assert [s["id"] for s in train] == [0, 1, 2, 3, 4, 5]
    assert [s["id"] for s in val] == [6, 7]
    assert [s["id"] for s in test] == [8, 9]


def test_load_jsonl(tmp_path: Path):
    """JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ­£ã—ãèª­ã¿è¾¼ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª"""
    jsonl_file = tmp_path / "test.jsonl"
    jsonl_file.write_text(
        '{"sns_text": "ãƒ†ã‚¹ãƒˆ1", "emoji_string": "ğŸ˜Š"}\n'
        '{"sns_text": "ãƒ†ã‚¹ãƒˆ2", "emoji_string": "ğŸ‰"}\n',
        encoding="utf-8",
    )

    data = load_jsonl(jsonl_file)
    assert len(data) == 2
    assert data[0]["sns_text"] == "ãƒ†ã‚¹ãƒˆ1"
    assert data[1]["emoji_string"] == "ğŸ‰"


def test_load_jsonl_empty(tmp_path: Path):
    """ç©ºã®JSONLãƒ•ã‚¡ã‚¤ãƒ«ã¯ç©ºãƒªã‚¹ãƒˆã‚’è¿”ã™"""
    jsonl_file = tmp_path / "empty.jsonl"
    jsonl_file.write_text("", encoding="utf-8")

    data = load_jsonl(jsonl_file)
    assert data == []


class TestTrainConfig:
    """TrainConfigã®ãƒ†ã‚¹ãƒˆ"""

    def test_default_values(self):
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ãŒæ­£ã—ãè¨­å®šã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª"""
        cfg = TrainConfig(model_name="test-model", output_dir="/tmp/test")
        assert cfg.model_name == "test-model"
        assert cfg.output_dir == "/tmp/test"
        assert cfg.num_train_epochs == 10
        assert cfg.fp16 is True
        assert cfg.label_smoothing_factor == 0.0
        assert cfg.early_stopping_patience is None
        assert cfg.save_total_limit is None
        assert cfg.report_to == "none"

    def test_custom_values(self):
        """ã‚«ã‚¹ã‚¿ãƒ å€¤ãŒæ­£ã—ãè¨­å®šã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª"""
        cfg = TrainConfig(
            model_name="test-model",
            output_dir="/tmp/test",
            num_train_epochs=50,
            fp16=False,
            label_smoothing_factor=0.1,
            early_stopping_patience=5,
            save_total_limit=3,
        )
        assert cfg.num_train_epochs == 50
        assert cfg.fp16 is False
        assert cfg.label_smoothing_factor == 0.1
        assert cfg.early_stopping_patience == 5
        assert cfg.save_total_limit == 3


class TestEvaluationResult:
    """EvaluationResultã®ãƒ†ã‚¹ãƒˆ"""

    def test_dataclass_fields(self):
        """å…¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒæ­£ã—ãè¨­å®šã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª"""
        result = EvaluationResult(
            avg_jaccard=0.5,
            exact_match_rate=0.1,
            micro_f1=0.4,
            avg_precision=0.6,
            avg_recall=0.7,
            avg_f1=0.65,
            num_samples=100,
            details=[{"text": "test"}],
        )
        assert result.avg_jaccard == 0.5
        assert result.exact_match_rate == 0.1
        assert result.micro_f1 == 0.4
        assert result.avg_precision == 0.6
        assert result.avg_recall == 0.7
        assert result.avg_f1 == 0.65
        assert result.num_samples == 100
        assert len(result.details) == 1


class TestGenerateEmoji:
    """generate_emojié–¢æ•°ã®ãƒ†ã‚¹ãƒˆ"""

    def test_generate_emoji_sampling(self):
        """sampling modeã§ã®ç”ŸæˆãŒå‹•ä½œã™ã‚‹ã“ã¨ã‚’ç¢ºèª"""
        mock_model = MagicMock()
        mock_tokenizer = MagicMock()

        # ãƒ¢ãƒƒã‚¯ã®è¨­å®š
        mock_tokenizer.return_value = {
            "input_ids": torch.tensor([[1, 2, 3]]),
            "attention_mask": torch.tensor([[1, 1, 1]]),
        }
        mock_model.generate.return_value = torch.tensor([[4, 5, 6]])
        mock_tokenizer.decode.return_value = "ğŸ˜Š ğŸ‰"

        result = generate_emoji(
            mock_model, mock_tokenizer, "ãƒ†ã‚¹ãƒˆ", use_sampling=True, device="cpu"
        )

        assert result == "ğŸ˜Š ğŸ‰"
        mock_model.eval.assert_called_once()
        mock_model.generate.assert_called_once()
        # sampling=Trueãªã®ã§do_sample=TrueãŒæ¸¡ã•ã‚Œã‚‹
        call_kwargs = mock_model.generate.call_args[1]
        assert call_kwargs["do_sample"] is True

    def test_generate_emoji_beam_search(self):
        """beam search modeã§ã®ç”ŸæˆãŒå‹•ä½œã™ã‚‹ã“ã¨ã‚’ç¢ºèª"""
        mock_model = MagicMock()
        mock_tokenizer = MagicMock()

        mock_tokenizer.return_value = {
            "input_ids": torch.tensor([[1, 2, 3]]),
            "attention_mask": torch.tensor([[1, 1, 1]]),
        }
        mock_model.generate.return_value = torch.tensor([[4, 5, 6]])
        mock_tokenizer.decode.return_value = "ğŸ˜Š"

        result = generate_emoji(
            mock_model, mock_tokenizer, "ãƒ†ã‚¹ãƒˆ", use_sampling=False, device="cpu"
        )

        assert result == "ğŸ˜Š"
        call_kwargs = mock_model.generate.call_args[1]
        assert "num_beams" in call_kwargs
        assert call_kwargs["num_beams"] == 4


class TestEvaluateModel:
    """evaluate_modelé–¢æ•°ã®ãƒ†ã‚¹ãƒˆ"""

    def test_evaluate_model_basic(self):
        """åŸºæœ¬çš„ãªè©•ä¾¡ãŒå‹•ä½œã™ã‚‹ã“ã¨ã‚’ç¢ºèª"""
        mock_model = MagicMock()
        mock_tokenizer = MagicMock()

        # generate_emojiã‚’ãƒ¢ãƒƒã‚¯
        with patch("src.models.t5_trainer.generate_emoji") as mock_gen:
            mock_gen.side_effect = ["ğŸ˜Š ğŸ‰", "ğŸ“š"]

            samples = [
                {"sns_text": "æ¥½ã—ã„", "emoji_string": "ğŸ˜Š ğŸ‰"},
                {"sns_text": "å‹‰å¼·", "emoji_string": "ğŸ“š âœï¸"},
            ]

            result = evaluate_model(mock_model, mock_tokenizer, samples, device="cpu")

            assert result.num_samples == 2
            assert len(result.details) == 2
            # 1ã¤ç›®ã¯å®Œå…¨ä¸€è‡´
            assert result.details[0]["exact_match"] is True
            # 2ã¤ç›®ã¯éƒ¨åˆ†ä¸€è‡´
            assert result.details[1]["exact_match"] is False

    def test_evaluate_model_max_samples(self):
        """max_samplesã§è©•ä¾¡æ•°ã‚’åˆ¶é™ã§ãã‚‹ã“ã¨ã‚’ç¢ºèª"""
        mock_model = MagicMock()
        mock_tokenizer = MagicMock()

        with patch("src.models.t5_trainer.generate_emoji") as mock_gen:
            mock_gen.return_value = "ğŸ˜Š"

            samples = [
                {"sns_text": f"test{i}", "emoji_string": "ğŸ˜Š"} for i in range(10)
            ]

            result = evaluate_model(
                mock_model, mock_tokenizer, samples, max_samples=3, device="cpu"
            )

            assert result.num_samples == 3
            assert mock_gen.call_count == 3

    def test_evaluate_model_empty_samples(self):
        """ç©ºã®ã‚µãƒ³ãƒ—ãƒ«ãƒªã‚¹ãƒˆã§è©•ä¾¡ã—ãŸå ´åˆ"""
        mock_model = MagicMock()
        mock_tokenizer = MagicMock()

        result = evaluate_model(mock_model, mock_tokenizer, [], device="cpu")

        assert result.num_samples == 0
        assert result.avg_jaccard == 0.0
        assert result.details == []


class TestBuildTrainer:
    """build_traineré–¢æ•°ã®ãƒ†ã‚¹ãƒˆ"""

    @pytest.fixture
    def mock_components(self):
        """ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ¢ãƒƒã‚¯ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ"""
        mock_model = MagicMock()
        mock_tokenizer = MagicMock()
        mock_train_dataset = MagicMock()
        mock_eval_dataset = MagicMock()
        return mock_model, mock_tokenizer, mock_train_dataset, mock_eval_dataset

    def test_build_trainer_without_early_stopping(self, mock_components, tmp_path):
        """EarlyStoppingãªã—ã§Trainerã‚’æ§‹ç¯‰"""
        model, tokenizer, train_ds, eval_ds = mock_components

        cfg = TrainConfig(
            model_name="test",
            output_dir=str(tmp_path),
            early_stopping_patience=None,
        )

        with patch("src.models.t5_trainer.Trainer") as mock_trainer_cls:
            with patch("src.models.t5_trainer.TrainingArguments"):
                with patch("src.models.t5_trainer.DataCollatorForSeq2Seq"):
                    build_trainer(model, tokenizer, train_ds, eval_ds, cfg)

                    # callbacksãŒNoneã¾ãŸã¯ç©ºã§å‘¼ã°ã‚Œã‚‹
                    call_kwargs = mock_trainer_cls.call_args[1]
                    assert (
                        call_kwargs["callbacks"] is None
                        or call_kwargs["callbacks"] == []
                    )

    def test_build_trainer_with_early_stopping(self, mock_components, tmp_path):
        """EarlyStoppingã‚ã‚Šã§Trainerã‚’æ§‹ç¯‰"""
        model, tokenizer, train_ds, eval_ds = mock_components

        cfg = TrainConfig(
            model_name="test",
            output_dir=str(tmp_path),
            early_stopping_patience=5,
        )

        with patch("src.models.t5_trainer.Trainer") as mock_trainer_cls:
            with patch("src.models.t5_trainer.TrainingArguments"):
                with patch("src.models.t5_trainer.DataCollatorForSeq2Seq"):
                    with patch(
                        "src.models.t5_trainer.EarlyStoppingCallback"
                    ) as mock_es:
                        build_trainer(model, tokenizer, train_ds, eval_ds, cfg)

                        # EarlyStoppingCallbackãŒä½œæˆã•ã‚Œã‚‹
                        mock_es.assert_called_once_with(early_stopping_patience=5)

                        # callbacksã«EarlyStoppingCallbackãŒå«ã¾ã‚Œã‚‹
                        call_kwargs = mock_trainer_cls.call_args[1]
                        assert call_kwargs["callbacks"] is not None
                        assert len(call_kwargs["callbacks"]) == 1
